{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Getting Started \u00b6 Dude is a very simple framework for writing web scrapers using Python decorators. The design, inspired by Flask , was to easily build a web scraper in just a few lines of code. Dude has an easy-to-learn syntax. Warning \ud83d\udea8 Dude is currently in Pre-Alpha. Please expect breaking changes. Installation \u00b6 To install, simply run the following from terminal. Click on the annotations (+ sign) for more details. Terminal pip install pydude #(1) playwright install #(2) Install pydude from PyPI Install playwright binaries for Chrome, Firefox and Webkit. See Getting Started | Playwright Python Minimal web scraper \u00b6 The simplest web scraper will look like the example below. Click on the annotations (+ sign) for more details. Python from dude import select #(1) @select ( css = \"a\" ) #(2) def get_link ( element ): #(3) return { \"url\" : element . get_attribute ( \"href\" )} #(4) Import the @select() decorator Decorate the function get_link() with @select() and specify the selector for finding the element in the page. It is required that decorator functions should accept 1 argument (2 for Pyppeteer). This can be an object or a string depending on which backend was used. Return a dictionary with information obtained from the argument object. The dictionary can contain multiple key-value pairs or can be empty. The example above will get all the hyperlink elements in a page and calls the handler function get_link() for each element. How to run the scraper \u00b6 To start scraping, use any of the following options. Click on the annotations (+ sign) for more details. Terminal Python dude scrape --url \"<url>\" --output data.json path/to/script.py #(1) You can run your scraper from terminal/shell/command-line by supplying URLs, the output filename of your choice and the paths to your python scripts to dude scrape command. if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ]) #(1) You can also use dude.run() function and run python path/to/script.py from terminal. The output in data.json should contain the actual URL and the metadata prepended with underscore. [ { \"_page_number\": 1, \"_page_url\": \"https://dude.ron.sh/\", \"_group_id\": 4502003824, \"_group_index\": 0, \"_element_index\": 0, \"url\": \"/url-1.html\" }, { \"_page_number\": 1, \"_page_url\": \"https://dude.ron.sh/\", \"_group_id\": 4502003824, \"_group_index\": 0, \"_element_index\": 1, \"url\": \"/url-2.html\" }, { \"_page_number\": 1, \"_page_url\": \"https://dude.ron.sh/\", \"_group_id\": 4502003824, \"_group_index\": 0, \"_element_index\": 2, \"url\": \"/url-3.html\" } ] Changing the output to --output data.csv should result in the following CSV content. License \u00b6 This project is licensed under the terms of the GNU AGPLv3+ license.","title":"Getting Started"},{"location":"index.html#getting-started","text":"Dude is a very simple framework for writing web scrapers using Python decorators. The design, inspired by Flask , was to easily build a web scraper in just a few lines of code. Dude has an easy-to-learn syntax. Warning \ud83d\udea8 Dude is currently in Pre-Alpha. Please expect breaking changes.","title":"Getting Started"},{"location":"index.html#installation","text":"To install, simply run the following from terminal. Click on the annotations (+ sign) for more details. Terminal pip install pydude #(1) playwright install #(2) Install pydude from PyPI Install playwright binaries for Chrome, Firefox and Webkit. See Getting Started | Playwright Python","title":"Installation"},{"location":"index.html#minimal-web-scraper","text":"The simplest web scraper will look like the example below. Click on the annotations (+ sign) for more details. Python from dude import select #(1) @select ( css = \"a\" ) #(2) def get_link ( element ): #(3) return { \"url\" : element . get_attribute ( \"href\" )} #(4) Import the @select() decorator Decorate the function get_link() with @select() and specify the selector for finding the element in the page. It is required that decorator functions should accept 1 argument (2 for Pyppeteer). This can be an object or a string depending on which backend was used. Return a dictionary with information obtained from the argument object. The dictionary can contain multiple key-value pairs or can be empty. The example above will get all the hyperlink elements in a page and calls the handler function get_link() for each element.","title":"Minimal web scraper"},{"location":"index.html#how-to-run-the-scraper","text":"To start scraping, use any of the following options. Click on the annotations (+ sign) for more details. Terminal Python dude scrape --url \"<url>\" --output data.json path/to/script.py #(1) You can run your scraper from terminal/shell/command-line by supplying URLs, the output filename of your choice and the paths to your python scripts to dude scrape command. if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ]) #(1) You can also use dude.run() function and run python path/to/script.py from terminal. The output in data.json should contain the actual URL and the metadata prepended with underscore. [ { \"_page_number\": 1, \"_page_url\": \"https://dude.ron.sh/\", \"_group_id\": 4502003824, \"_group_index\": 0, \"_element_index\": 0, \"url\": \"/url-1.html\" }, { \"_page_number\": 1, \"_page_url\": \"https://dude.ron.sh/\", \"_group_id\": 4502003824, \"_group_index\": 0, \"_element_index\": 1, \"url\": \"/url-2.html\" }, { \"_page_number\": 1, \"_page_url\": \"https://dude.ron.sh/\", \"_group_id\": 4502003824, \"_group_index\": 0, \"_element_index\": 2, \"url\": \"/url-3.html\" } ] Changing the output to --output data.csv should result in the following CSV content.","title":"How to run the scraper"},{"location":"index.html#license","text":"This project is licensed under the terms of the GNU AGPLv3+ license.","title":"License"},{"location":"basic_usage.html","text":"Basic Usage \u00b6 To use dude , start by importing the library. Python from dude import select A basic handler function consists of the structure below. A handler function should accept 1 argument (element) and should be decorated with @select() . The handler should return a dictionary. Click on the annotations (+ sign) for more details. Python @select ( css = \"<put-your-selector-here>\" ) # (1) def handler ( element ): # (2) ... # (3) return { \"<key>\" : \"<value-extracted-from-element>\" } # (4) @select() decorator. Function should accept 1 parameter, the element object found in the page being scraped. You can specify your Python algorithm here. Return a dictionary. This can contain an arbitrary amount of key-value pairs. The example handler below extracts the text content of any element that matches the CSS selector .title . Python from dude import select @select ( css = \".title\" ) def result_title ( element ): \"\"\" Result title. \"\"\" return { \"title\" : element . text_content ()} It is possible to attach a single handler to multiple selectors. Python from dude import select @select ( css = \"<a-selector>\" ) @select ( selector = \"<another-selector>\" ) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } Supported selector types \u00b6 The @select() decorator does not only accept selector but also css , xpath , text and regex . Please take note that css , xpath , text and regex are specific and selector can contain any of these types. Python from dude import select @select ( css = \"<css-selector>\" ) #(1) @select ( xpath = \"<xpath-selector>\" ) #(2) @select ( text = \"<text-selector>\" ) #(3) @select ( regex = \"<regex-selector>\" ) #(4) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } CSS Selector XPath Selector Text Selector Regular Expression Selector It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence selector -> css -> xpath -> text -> regex . How to run the scraper \u00b6 To start scraping, use any of the following options. Click on the annotations (+ sign) for more details. Terminal Python dude scrape --url \"<url>\" --output data.json path/to/script.py #(1) You can run your scraper from terminal/shell/command-line by supplying URLs, the output filename of your choice and the paths to your python scripts to dude scrape command. if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ]) #(1) You can also use dude.run() function and run python path/to/script.py from terminal. Examples \u00b6 Check out the example in examples/flat.py and run it on your terminal using the command python examples/flat.py .","title":"Basic Usage"},{"location":"basic_usage.html#basic-usage","text":"To use dude , start by importing the library. Python from dude import select A basic handler function consists of the structure below. A handler function should accept 1 argument (element) and should be decorated with @select() . The handler should return a dictionary. Click on the annotations (+ sign) for more details. Python @select ( css = \"<put-your-selector-here>\" ) # (1) def handler ( element ): # (2) ... # (3) return { \"<key>\" : \"<value-extracted-from-element>\" } # (4) @select() decorator. Function should accept 1 parameter, the element object found in the page being scraped. You can specify your Python algorithm here. Return a dictionary. This can contain an arbitrary amount of key-value pairs. The example handler below extracts the text content of any element that matches the CSS selector .title . Python from dude import select @select ( css = \".title\" ) def result_title ( element ): \"\"\" Result title. \"\"\" return { \"title\" : element . text_content ()} It is possible to attach a single handler to multiple selectors. Python from dude import select @select ( css = \"<a-selector>\" ) @select ( selector = \"<another-selector>\" ) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" }","title":"Basic Usage"},{"location":"basic_usage.html#supported-selector-types","text":"The @select() decorator does not only accept selector but also css , xpath , text and regex . Please take note that css , xpath , text and regex are specific and selector can contain any of these types. Python from dude import select @select ( css = \"<css-selector>\" ) #(1) @select ( xpath = \"<xpath-selector>\" ) #(2) @select ( text = \"<text-selector>\" ) #(3) @select ( regex = \"<regex-selector>\" ) #(4) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } CSS Selector XPath Selector Text Selector Regular Expression Selector It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence selector -> css -> xpath -> text -> regex .","title":"Supported selector types"},{"location":"basic_usage.html#how-to-run-the-scraper","text":"To start scraping, use any of the following options. Click on the annotations (+ sign) for more details. Terminal Python dude scrape --url \"<url>\" --output data.json path/to/script.py #(1) You can run your scraper from terminal/shell/command-line by supplying URLs, the output filename of your choice and the paths to your python scripts to dude scrape command. if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ]) #(1) You can also use dude.run() function and run python path/to/script.py from terminal.","title":"How to run the scraper"},{"location":"basic_usage.html#examples","text":"Check out the example in examples/flat.py and run it on your terminal using the command python examples/flat.py .","title":"Examples"},{"location":"cli.html","text":"Command-Line Interface (CLI) \u00b6 CLI usage: dude scrape [-h] [--url URL] [--playwright | --bs4 | --parsel | --lxml | --pyppeteer | --selenium] [--headed] [--browser {chromium,firefox,webkit}] [--pages PAGES] [--output OUTPUT] [--format FORMAT] [--proxy-server PROXY_SERVER] [--proxy-user PROXY_USER] [--proxy-pass PROXY_PASS] [--follow-urls] [--save-per-page] [--ignore-robots-txt] PATH [PATH ...] Run the dude scraper. optional arguments: -h, --help show this help message and exit required arguments: PATH Path to python file/s containing the handler functions. --url URL Website URL to scrape. Accepts one or more url (e.g. \"dude scrape --url <url1> --url <url2> ...\") optional arguments: --playwright Use Playwright. --bs4 Use BeautifulSoup4. --parsel Use Parsel. --lxml Use lxml. --pyppeteer Use Pyppeteer. --selenium Use Selenium. --headed Run headed browser. --browser {chromium,firefox,webkit} Browser type to use. --pages PAGES Maximum number of pages to crawl before exiting (default=1). This is only valid when a navigate handler is defined. --output OUTPUT Output file. If not provided, prints into the terminal. --format FORMAT Output file format. If not provided, uses the extension of the output file or defaults to \"json\". Supports \"json\", \"yaml/yml\", and \"csv\" but can be extended using the @save() decorator. --proxy-server PROXY_SERVER Proxy server. --proxy-user PROXY_USER Proxy username. --proxy-pass PROXY_PASS Proxy password. --follow-urls Automatically follow URLs. --save-per-page Flag to save data on every page extraction or not. If not, saves all the data at the end.If --follow-urls is set to true, this variable will be automatically set to true. --ignore-robots-txt Flag to ignore robots.txt.","title":"Command-Line Interface (CLI)"},{"location":"cli.html#command-line-interface-cli","text":"CLI usage: dude scrape [-h] [--url URL] [--playwright | --bs4 | --parsel | --lxml | --pyppeteer | --selenium] [--headed] [--browser {chromium,firefox,webkit}] [--pages PAGES] [--output OUTPUT] [--format FORMAT] [--proxy-server PROXY_SERVER] [--proxy-user PROXY_USER] [--proxy-pass PROXY_PASS] [--follow-urls] [--save-per-page] [--ignore-robots-txt] PATH [PATH ...] Run the dude scraper. optional arguments: -h, --help show this help message and exit required arguments: PATH Path to python file/s containing the handler functions. --url URL Website URL to scrape. Accepts one or more url (e.g. \"dude scrape --url <url1> --url <url2> ...\") optional arguments: --playwright Use Playwright. --bs4 Use BeautifulSoup4. --parsel Use Parsel. --lxml Use lxml. --pyppeteer Use Pyppeteer. --selenium Use Selenium. --headed Run headed browser. --browser {chromium,firefox,webkit} Browser type to use. --pages PAGES Maximum number of pages to crawl before exiting (default=1). This is only valid when a navigate handler is defined. --output OUTPUT Output file. If not provided, prints into the terminal. --format FORMAT Output file format. If not provided, uses the extension of the output file or defaults to \"json\". Supports \"json\", \"yaml/yml\", and \"csv\" but can be extended using the @save() decorator. --proxy-server PROXY_SERVER Proxy server. --proxy-user PROXY_USER Proxy username. --proxy-pass PROXY_PASS Proxy password. --follow-urls Automatically follow URLs. --save-per-page Flag to save data on every page extraction or not. If not, saves all the data at the end.If --follow-urls is set to true, this variable will be automatically set to true. --ignore-robots-txt Flag to ignore robots.txt.","title":"Command-Line Interface (CLI)"},{"location":"features.html","text":"Features \u00b6 Simple Flask -inspired design - build a scraper with decorators. Uses Playwright API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc. Data grouping - group related results. URL pattern matching - run functions on matched URLs. Priority - reorder functions based on priority. Setup function - enable setup steps (clicking dialogs or login). Navigate function - enable navigation steps to move to other pages. Custom storage - option to save data to other formats or database. Async support - write async handlers. Option to use other parser backends aside from Playwright. BeautifulSoup4 - pip install pydude[bs4] Parsel - pip install pydude[parsel] lxml - pip install pydude[lxml] Pyppeteer - pip install pydude[pyppeteer] Selenium - pip install pydude[selenium] Option to follow all links indefinitely (Crawler/Spider). Events - attach functions to startup, pre-setup, post-setup and shutdown events. Option to save data on every page.","title":"Features"},{"location":"features.html#features","text":"Simple Flask -inspired design - build a scraper with decorators. Uses Playwright API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc. Data grouping - group related results. URL pattern matching - run functions on matched URLs. Priority - reorder functions based on priority. Setup function - enable setup steps (clicking dialogs or login). Navigate function - enable navigation steps to move to other pages. Custom storage - option to save data to other formats or database. Async support - write async handlers. Option to use other parser backends aside from Playwright. BeautifulSoup4 - pip install pydude[bs4] Parsel - pip install pydude[parsel] lxml - pip install pydude[lxml] Pyppeteer - pip install pydude[pyppeteer] Selenium - pip install pydude[selenium] Option to follow all links indefinitely (Crawler/Spider). Events - attach functions to startup, pre-setup, post-setup and shutdown events. Option to save data on every page.","title":"Features"},{"location":"reference.html","text":"Scraper class \u00b6 Convenience class to easily use the available decorators. Source code in dude/scraper.py class Scraper ( ScraperBase ): \"\"\" Convenience class to easily use the available decorators. \"\"\" def run ( self , urls : Sequence [ str ], pages : int = 1 , proxy : Optional [ Any ] = None , output : Optional [ str ] = None , format : str = \"json\" , follow_urls : bool = False , save_per_page : bool = False , ignore_robots_txt : bool = False , # extra args parser : str = \"playwright\" , headless : bool = True , browser_type : str = \"chromium\" , ** kwargs : Any , ) -> None : \"\"\" Convenience method to handle switching between different types of parser backends. :param urls: List of website URLs. :param pages: Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa :param proxy: Proxy settings. :param output: Output file. If not provided, prints in the terminal. :param format: Output file format. If not provided, uses the extension of the output file or defaults to json. :param follow_urls: Automatically follow URLs. :param save_per_page: Flag to save data on every page extraction or not. If not, saves all the data at the end. :param ignore_robots_txt: Flag to ignore robots.txt. :param parser: Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\", \"pyppeteer\" or \"selenium\"] :param headless: Enables headless browser. (default=True) :param browser_type: Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\"). \"\"\" logger . info ( \"Scraper started...\" ) if not self . scraper : scraper_class : Type [ ScraperBase ] if parser == \"bs4\" : from .optional.beautifulsoup_scraper import BeautifulSoupScraper scraper_class = BeautifulSoupScraper elif parser == \"parsel\" : from .optional.parsel_scraper import ParselScraper scraper_class = ParselScraper elif parser == \"lxml\" : from .optional.lxml_scraper import LxmlScraper scraper_class = LxmlScraper elif parser == \"pyppeteer\" : from .optional.pyppeteer_scraper import PyppeteerScraper scraper_class = PyppeteerScraper elif parser == \"selenium\" : from .optional.selenium_scraper import SeleniumScraper scraper_class = SeleniumScraper else : scraper_class = PlaywrightScraper self . scraper = scraper_class ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , events = self . events , has_async = self . has_async , requests = self . requests , ) self . scraper . run ( urls = urls , pages = pages , proxy = proxy , output = output , format = format , follow_urls = follow_urls , save_per_page = save_per_page or follow_urls , ignore_robots_txt = ignore_robots_txt , ** { \"headless\" : headless , \"browser_type\" : browser_type }, ) event_shutdown ( self ) inherited \u00b6 Run all shutdown events Source code in dude/scraper.py def event_shutdown ( self ) -> None : \"\"\" Run all shutdown events \"\"\" self . run_event ( \"shutdown\" ) event_startup ( self ) inherited \u00b6 Run all startup events Source code in dude/scraper.py def event_startup ( self ) -> None : \"\"\" Run all startup events \"\"\" self . run_event ( \"startup\" ) group ( self , selector = None , css = None , xpath = None , text = None , regex = None ) inherited \u00b6 Decorator to register a handler function to a given group. Parameters: Name Type Description Default selector str Element selector (any of CSS, XPath, text, regex). None css str CSS selector. None xpath str XPath selector. None text str Text selector. None regex str Regular expression selector None Source code in dude/scraper.py def group ( self , selector : str = None , css : str = None , xpath : str = None , text : str = None , regex : str = None , ) -> Callable : \"\"\" Decorator to register a handler function to a given group. :param selector: Element selector (any of CSS, XPath, text, regex). :param css: CSS selector. :param xpath: XPath selector. :param text: Text selector. :param regex: Regular expression selector \"\"\" def wrapper ( func : Callable ) -> Union [ Callable , Coroutine ]: if not ( selector or css or xpath or text or regex ): raise Exception ( \"Any of selector, css, xpath, text or regex selectors must be present\" ) if asyncio . iscoroutinefunction ( func ): self . has_async = True group = Selector ( selector = selector , css = css , xpath = xpath , text = text , regex = regex ) if not self . scraper : if func in self . groups : logger . warning ( \"Group ' %s ' already exists for function ' %s '. Skipping ' %s '...\" , self . groups [ func ], func . __name__ , group , ) else : self . groups [ func ] = group else : if func in self . scraper . groups : logger . warning ( \"Group ' %s ' already exists for function ' %s '. Skipping ' %s '...\" , self . scraper . groups [ func ], func . __name__ , group , ) else : self . scraper . groups [ func ] = group return func return wrapper post_setup ( self ) inherited \u00b6 Decorator to register a function to post-setup events. Post-setup events are executed after running the setup functions and before the actual web-scraping happens. This is useful when \"page clean-ups\" are done in the setup functions. Source code in dude/scraper.py def post_setup ( self ) -> Callable : \"\"\" Decorator to register a function to post-setup events. Post-setup events are executed after running the setup functions and before the actual web-scraping happens. This is useful when \"page clean-ups\" are done in the setup functions. \"\"\" def wrapper ( func : Callable ) -> Callable : if asyncio . iscoroutinefunction ( func ): self . has_async = True events = self . scraper . events if self . scraper else self . events events [ \"post-setup\" ] . append ( func ) return func return wrapper pre_setup ( self ) inherited \u00b6 Decorator to register a function to pre-setup events. Pre-setup events are executed after a page is loaded (or HTTP response in case of HTTPX) and before running the setup functions. Source code in dude/scraper.py def pre_setup ( self ) -> Callable : \"\"\" Decorator to register a function to pre-setup events. Pre-setup events are executed after a page is loaded (or HTTP response in case of HTTPX) and before running the setup functions. \"\"\" def wrapper ( func : Callable ) -> Callable : if asyncio . iscoroutinefunction ( func ): self . has_async = True events = self . scraper . events if self . scraper else self . events events [ \"pre-setup\" ] . append ( func ) return func return wrapper run ( self , urls , pages = 1 , proxy = None , output = None , format = 'json' , follow_urls = False , save_per_page = False , ignore_robots_txt = False , parser = 'playwright' , headless = True , browser_type = 'chromium' , ** kwargs ) \u00b6 Convenience method to handle switching between different types of parser backends. Parameters: Name Type Description Default urls Sequence[str] List of website URLs. required pages int Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa 1 proxy Optional[Any] Proxy settings. None output Optional[str] Output file. If not provided, prints in the terminal. None format str Output file format. If not provided, uses the extension of the output file or defaults to json. 'json' follow_urls bool Automatically follow URLs. False save_per_page bool Flag to save data on every page extraction or not. If not, saves all the data at the end. False ignore_robots_txt bool Flag to ignore robots.txt. False parser str Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\", \"pyppeteer\" or \"selenium\"] 'playwright' headless bool Enables headless browser. (default=True) True browser_type str Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\"). 'chromium' Source code in dude/scraper.py def run ( self , urls : Sequence [ str ], pages : int = 1 , proxy : Optional [ Any ] = None , output : Optional [ str ] = None , format : str = \"json\" , follow_urls : bool = False , save_per_page : bool = False , ignore_robots_txt : bool = False , # extra args parser : str = \"playwright\" , headless : bool = True , browser_type : str = \"chromium\" , ** kwargs : Any , ) -> None : \"\"\" Convenience method to handle switching between different types of parser backends. :param urls: List of website URLs. :param pages: Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa :param proxy: Proxy settings. :param output: Output file. If not provided, prints in the terminal. :param format: Output file format. If not provided, uses the extension of the output file or defaults to json. :param follow_urls: Automatically follow URLs. :param save_per_page: Flag to save data on every page extraction or not. If not, saves all the data at the end. :param ignore_robots_txt: Flag to ignore robots.txt. :param parser: Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\", \"pyppeteer\" or \"selenium\"] :param headless: Enables headless browser. (default=True) :param browser_type: Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\"). \"\"\" logger . info ( \"Scraper started...\" ) if not self . scraper : scraper_class : Type [ ScraperBase ] if parser == \"bs4\" : from .optional.beautifulsoup_scraper import BeautifulSoupScraper scraper_class = BeautifulSoupScraper elif parser == \"parsel\" : from .optional.parsel_scraper import ParselScraper scraper_class = ParselScraper elif parser == \"lxml\" : from .optional.lxml_scraper import LxmlScraper scraper_class = LxmlScraper elif parser == \"pyppeteer\" : from .optional.pyppeteer_scraper import PyppeteerScraper scraper_class = PyppeteerScraper elif parser == \"selenium\" : from .optional.selenium_scraper import SeleniumScraper scraper_class = SeleniumScraper else : scraper_class = PlaywrightScraper self . scraper = scraper_class ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , events = self . events , has_async = self . has_async , requests = self . requests , ) self . scraper . run ( urls = urls , pages = pages , proxy = proxy , output = output , format = format , follow_urls = follow_urls , save_per_page = save_per_page or follow_urls , ignore_robots_txt = ignore_robots_txt , ** { \"headless\" : headless , \"browser_type\" : browser_type }, ) save ( self , format , is_per_page = False ) inherited \u00b6 Decorator to register a save function to a format. Parameters: Name Type Description Default format str Format (json, csv, or any custom string). required is_per_page bool Flag to identify if func will be called after each page. False Source code in dude/scraper.py def save ( self , format : str , is_per_page : bool = False ) -> Callable : \"\"\" Decorator to register a save function to a format. :param format: Format (json, csv, or any custom string). :param is_per_page: Flag to identify if func will be called after each page. \"\"\" def wrapper ( func : Callable ) -> Callable : if asyncio . iscoroutinefunction ( func ): self . has_async = True save_rules = self . scraper . save_rules if self . scraper else self . save_rules save_rules [ format , is_per_page ] = func return func return wrapper select ( self , selector = None , group = None , setup = False , navigate = False , url_match = '*' , priority = 100 , css = None , xpath = None , text = None , regex = None , group_css = None , group_xpath = None , group_text = None , group_regex = None ) inherited \u00b6 Decorator to register a handler function to a given selector. Parameters: Name Type Description Default selector str Element selector (CSS, XPath, text, regex). None group str (Optional) Element selector where the matched element should be grouped. Defaults to \":root\". None setup bool Flag to register a setup handler. False navigate bool Flag to register a navigate handler. False url_match Union[str, Callable] URL pattern matcher. Run the handler function only when the pattern matches (defaults to *) or when custom function/lambda returns True. # noqa '*' priority int Priority, the lowest value will be executed first (default 100). 100 css str CSS selector. None xpath str XPath selector. None text str Text selector. None regex str Regular expression selector None group_css str Group CSS selector. None group_xpath str Group XPath selector. None group_text str Group Text selector. None group_regex str Group Regular expression selector None Source code in dude/scraper.py def select ( self , selector : str = None , group : str = None , setup : bool = False , navigate : bool = False , url_match : Union [ str , Callable ] = \"*\" , priority : int = 100 , css : str = None , xpath : str = None , text : str = None , regex : str = None , group_css : str = None , group_xpath : str = None , group_text : str = None , group_regex : str = None , ) -> Callable : \"\"\" Decorator to register a handler function to a given selector. :param selector: Element selector (CSS, XPath, text, regex). :param group: (Optional) Element selector where the matched element should be grouped. Defaults to \":root\". :param setup: Flag to register a setup handler. :param navigate: Flag to register a navigate handler. :param url_match: URL pattern matcher. Run the handler function only when the pattern matches (defaults to *) or when custom function/lambda returns True. # noqa :param priority: Priority, the lowest value will be executed first (default 100). :param css: CSS selector. :param xpath: XPath selector. :param text: Text selector. :param regex: Regular expression selector :param group_css: Group CSS selector. :param group_xpath: Group XPath selector. :param group_text: Group Text selector. :param group_regex: Group Regular expression selector \"\"\" def wrapper ( func : Callable ) -> Union [ Callable , Coroutine ]: sel = Selector ( selector = selector , css = css , xpath = xpath , text = text , regex = regex ) assert sel , \"Any of selector, css, xpath, text and regex params should be present.\" if asyncio . iscoroutinefunction ( func ): self . has_async = True rule = Rule ( selector = sel , group = Selector ( selector = group , css = group_css , xpath = group_xpath , text = group_text , regex = group_regex ), url_matcher = url_match , handler = func , setup = setup , navigate = navigate , priority = priority , ) rules = self . scraper . rules if self . scraper else self . rules rules . append ( rule ) return func return wrapper shutdown ( self ) inherited \u00b6 Decorator to register a function to the shutdown events. Shutdown events are executed before terminating the application for cleaning up or closing resources like files and database sessions. Source code in dude/scraper.py def shutdown ( self ) -> Callable : \"\"\" Decorator to register a function to the shutdown events. Shutdown events are executed before terminating the application for cleaning up or closing resources like files and database sessions. \"\"\" def wrapper ( func : Callable ) -> Callable : if asyncio . iscoroutinefunction ( func ): self . has_async = True events = self . scraper . events if self . scraper else self . events events [ \"shutdown\" ] . append ( func ) return func return wrapper start_requests ( self ) inherited \u00b6 Decorator to register custom Request objects. Source code in dude/scraper.py def start_requests ( self ) -> Callable : \"\"\" Decorator to register custom Request objects. \"\"\" from httpx import Request def wrapper ( func : Callable ) -> Callable : requests = self . scraper . requests if self . scraper else self . requests for request in func (): assert isinstance ( request , Request ) requests . append ( request ) return func return wrapper startup ( self ) inherited \u00b6 Decorator to register a function to startup events. Startup events are executed before any actual scraping happens to, for example, setup databases, etc. Source code in dude/scraper.py def startup ( self ) -> Callable : \"\"\" Decorator to register a function to startup events. Startup events are executed before any actual scraping happens to, for example, setup databases, etc. \"\"\" def wrapper ( func : Callable ) -> Callable : if asyncio . iscoroutinefunction ( func ): self . has_async = True events = self . scraper . events if self . scraper else self . events events [ \"startup\" ] . append ( func ) return func return wrapper","title":"Scraper class"},{"location":"reference.html#scraper-class","text":"Convenience class to easily use the available decorators. Source code in dude/scraper.py class Scraper ( ScraperBase ): \"\"\" Convenience class to easily use the available decorators. \"\"\" def run ( self , urls : Sequence [ str ], pages : int = 1 , proxy : Optional [ Any ] = None , output : Optional [ str ] = None , format : str = \"json\" , follow_urls : bool = False , save_per_page : bool = False , ignore_robots_txt : bool = False , # extra args parser : str = \"playwright\" , headless : bool = True , browser_type : str = \"chromium\" , ** kwargs : Any , ) -> None : \"\"\" Convenience method to handle switching between different types of parser backends. :param urls: List of website URLs. :param pages: Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa :param proxy: Proxy settings. :param output: Output file. If not provided, prints in the terminal. :param format: Output file format. If not provided, uses the extension of the output file or defaults to json. :param follow_urls: Automatically follow URLs. :param save_per_page: Flag to save data on every page extraction or not. If not, saves all the data at the end. :param ignore_robots_txt: Flag to ignore robots.txt. :param parser: Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\", \"pyppeteer\" or \"selenium\"] :param headless: Enables headless browser. (default=True) :param browser_type: Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\"). \"\"\" logger . info ( \"Scraper started...\" ) if not self . scraper : scraper_class : Type [ ScraperBase ] if parser == \"bs4\" : from .optional.beautifulsoup_scraper import BeautifulSoupScraper scraper_class = BeautifulSoupScraper elif parser == \"parsel\" : from .optional.parsel_scraper import ParselScraper scraper_class = ParselScraper elif parser == \"lxml\" : from .optional.lxml_scraper import LxmlScraper scraper_class = LxmlScraper elif parser == \"pyppeteer\" : from .optional.pyppeteer_scraper import PyppeteerScraper scraper_class = PyppeteerScraper elif parser == \"selenium\" : from .optional.selenium_scraper import SeleniumScraper scraper_class = SeleniumScraper else : scraper_class = PlaywrightScraper self . scraper = scraper_class ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , events = self . events , has_async = self . has_async , requests = self . requests , ) self . scraper . run ( urls = urls , pages = pages , proxy = proxy , output = output , format = format , follow_urls = follow_urls , save_per_page = save_per_page or follow_urls , ignore_robots_txt = ignore_robots_txt , ** { \"headless\" : headless , \"browser_type\" : browser_type }, )","title":"Scraper class"},{"location":"reference.html#dude.scraper.Scraper.event_shutdown","text":"Run all shutdown events Source code in dude/scraper.py def event_shutdown ( self ) -> None : \"\"\" Run all shutdown events \"\"\" self . run_event ( \"shutdown\" )","title":"event_shutdown()"},{"location":"reference.html#dude.scraper.Scraper.event_startup","text":"Run all startup events Source code in dude/scraper.py def event_startup ( self ) -> None : \"\"\" Run all startup events \"\"\" self . run_event ( \"startup\" )","title":"event_startup()"},{"location":"reference.html#dude.scraper.Scraper.group","text":"Decorator to register a handler function to a given group. Parameters: Name Type Description Default selector str Element selector (any of CSS, XPath, text, regex). None css str CSS selector. None xpath str XPath selector. None text str Text selector. None regex str Regular expression selector None Source code in dude/scraper.py def group ( self , selector : str = None , css : str = None , xpath : str = None , text : str = None , regex : str = None , ) -> Callable : \"\"\" Decorator to register a handler function to a given group. :param selector: Element selector (any of CSS, XPath, text, regex). :param css: CSS selector. :param xpath: XPath selector. :param text: Text selector. :param regex: Regular expression selector \"\"\" def wrapper ( func : Callable ) -> Union [ Callable , Coroutine ]: if not ( selector or css or xpath or text or regex ): raise Exception ( \"Any of selector, css, xpath, text or regex selectors must be present\" ) if asyncio . iscoroutinefunction ( func ): self . has_async = True group = Selector ( selector = selector , css = css , xpath = xpath , text = text , regex = regex ) if not self . scraper : if func in self . groups : logger . warning ( \"Group ' %s ' already exists for function ' %s '. Skipping ' %s '...\" , self . groups [ func ], func . __name__ , group , ) else : self . groups [ func ] = group else : if func in self . scraper . groups : logger . warning ( \"Group ' %s ' already exists for function ' %s '. Skipping ' %s '...\" , self . scraper . groups [ func ], func . __name__ , group , ) else : self . scraper . groups [ func ] = group return func return wrapper","title":"group()"},{"location":"reference.html#dude.scraper.Scraper.post_setup","text":"Decorator to register a function to post-setup events. Post-setup events are executed after running the setup functions and before the actual web-scraping happens. This is useful when \"page clean-ups\" are done in the setup functions. Source code in dude/scraper.py def post_setup ( self ) -> Callable : \"\"\" Decorator to register a function to post-setup events. Post-setup events are executed after running the setup functions and before the actual web-scraping happens. This is useful when \"page clean-ups\" are done in the setup functions. \"\"\" def wrapper ( func : Callable ) -> Callable : if asyncio . iscoroutinefunction ( func ): self . has_async = True events = self . scraper . events if self . scraper else self . events events [ \"post-setup\" ] . append ( func ) return func return wrapper","title":"post_setup()"},{"location":"reference.html#dude.scraper.Scraper.pre_setup","text":"Decorator to register a function to pre-setup events. Pre-setup events are executed after a page is loaded (or HTTP response in case of HTTPX) and before running the setup functions. Source code in dude/scraper.py def pre_setup ( self ) -> Callable : \"\"\" Decorator to register a function to pre-setup events. Pre-setup events are executed after a page is loaded (or HTTP response in case of HTTPX) and before running the setup functions. \"\"\" def wrapper ( func : Callable ) -> Callable : if asyncio . iscoroutinefunction ( func ): self . has_async = True events = self . scraper . events if self . scraper else self . events events [ \"pre-setup\" ] . append ( func ) return func return wrapper","title":"pre_setup()"},{"location":"reference.html#dude.scraper.Scraper.run","text":"Convenience method to handle switching between different types of parser backends. Parameters: Name Type Description Default urls Sequence[str] List of website URLs. required pages int Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa 1 proxy Optional[Any] Proxy settings. None output Optional[str] Output file. If not provided, prints in the terminal. None format str Output file format. If not provided, uses the extension of the output file or defaults to json. 'json' follow_urls bool Automatically follow URLs. False save_per_page bool Flag to save data on every page extraction or not. If not, saves all the data at the end. False ignore_robots_txt bool Flag to ignore robots.txt. False parser str Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\", \"pyppeteer\" or \"selenium\"] 'playwright' headless bool Enables headless browser. (default=True) True browser_type str Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\"). 'chromium' Source code in dude/scraper.py def run ( self , urls : Sequence [ str ], pages : int = 1 , proxy : Optional [ Any ] = None , output : Optional [ str ] = None , format : str = \"json\" , follow_urls : bool = False , save_per_page : bool = False , ignore_robots_txt : bool = False , # extra args parser : str = \"playwright\" , headless : bool = True , browser_type : str = \"chromium\" , ** kwargs : Any , ) -> None : \"\"\" Convenience method to handle switching between different types of parser backends. :param urls: List of website URLs. :param pages: Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa :param proxy: Proxy settings. :param output: Output file. If not provided, prints in the terminal. :param format: Output file format. If not provided, uses the extension of the output file or defaults to json. :param follow_urls: Automatically follow URLs. :param save_per_page: Flag to save data on every page extraction or not. If not, saves all the data at the end. :param ignore_robots_txt: Flag to ignore robots.txt. :param parser: Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\", \"pyppeteer\" or \"selenium\"] :param headless: Enables headless browser. (default=True) :param browser_type: Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\"). \"\"\" logger . info ( \"Scraper started...\" ) if not self . scraper : scraper_class : Type [ ScraperBase ] if parser == \"bs4\" : from .optional.beautifulsoup_scraper import BeautifulSoupScraper scraper_class = BeautifulSoupScraper elif parser == \"parsel\" : from .optional.parsel_scraper import ParselScraper scraper_class = ParselScraper elif parser == \"lxml\" : from .optional.lxml_scraper import LxmlScraper scraper_class = LxmlScraper elif parser == \"pyppeteer\" : from .optional.pyppeteer_scraper import PyppeteerScraper scraper_class = PyppeteerScraper elif parser == \"selenium\" : from .optional.selenium_scraper import SeleniumScraper scraper_class = SeleniumScraper else : scraper_class = PlaywrightScraper self . scraper = scraper_class ( rules = self . rules , groups = self . groups , save_rules = self . save_rules , events = self . events , has_async = self . has_async , requests = self . requests , ) self . scraper . run ( urls = urls , pages = pages , proxy = proxy , output = output , format = format , follow_urls = follow_urls , save_per_page = save_per_page or follow_urls , ignore_robots_txt = ignore_robots_txt , ** { \"headless\" : headless , \"browser_type\" : browser_type }, )","title":"run()"},{"location":"reference.html#dude.scraper.Scraper.save","text":"Decorator to register a save function to a format. Parameters: Name Type Description Default format str Format (json, csv, or any custom string). required is_per_page bool Flag to identify if func will be called after each page. False Source code in dude/scraper.py def save ( self , format : str , is_per_page : bool = False ) -> Callable : \"\"\" Decorator to register a save function to a format. :param format: Format (json, csv, or any custom string). :param is_per_page: Flag to identify if func will be called after each page. \"\"\" def wrapper ( func : Callable ) -> Callable : if asyncio . iscoroutinefunction ( func ): self . has_async = True save_rules = self . scraper . save_rules if self . scraper else self . save_rules save_rules [ format , is_per_page ] = func return func return wrapper","title":"save()"},{"location":"reference.html#dude.scraper.Scraper.select","text":"Decorator to register a handler function to a given selector. Parameters: Name Type Description Default selector str Element selector (CSS, XPath, text, regex). None group str (Optional) Element selector where the matched element should be grouped. Defaults to \":root\". None setup bool Flag to register a setup handler. False navigate bool Flag to register a navigate handler. False url_match Union[str, Callable] URL pattern matcher. Run the handler function only when the pattern matches (defaults to *) or when custom function/lambda returns True. # noqa '*' priority int Priority, the lowest value will be executed first (default 100). 100 css str CSS selector. None xpath str XPath selector. None text str Text selector. None regex str Regular expression selector None group_css str Group CSS selector. None group_xpath str Group XPath selector. None group_text str Group Text selector. None group_regex str Group Regular expression selector None Source code in dude/scraper.py def select ( self , selector : str = None , group : str = None , setup : bool = False , navigate : bool = False , url_match : Union [ str , Callable ] = \"*\" , priority : int = 100 , css : str = None , xpath : str = None , text : str = None , regex : str = None , group_css : str = None , group_xpath : str = None , group_text : str = None , group_regex : str = None , ) -> Callable : \"\"\" Decorator to register a handler function to a given selector. :param selector: Element selector (CSS, XPath, text, regex). :param group: (Optional) Element selector where the matched element should be grouped. Defaults to \":root\". :param setup: Flag to register a setup handler. :param navigate: Flag to register a navigate handler. :param url_match: URL pattern matcher. Run the handler function only when the pattern matches (defaults to *) or when custom function/lambda returns True. # noqa :param priority: Priority, the lowest value will be executed first (default 100). :param css: CSS selector. :param xpath: XPath selector. :param text: Text selector. :param regex: Regular expression selector :param group_css: Group CSS selector. :param group_xpath: Group XPath selector. :param group_text: Group Text selector. :param group_regex: Group Regular expression selector \"\"\" def wrapper ( func : Callable ) -> Union [ Callable , Coroutine ]: sel = Selector ( selector = selector , css = css , xpath = xpath , text = text , regex = regex ) assert sel , \"Any of selector, css, xpath, text and regex params should be present.\" if asyncio . iscoroutinefunction ( func ): self . has_async = True rule = Rule ( selector = sel , group = Selector ( selector = group , css = group_css , xpath = group_xpath , text = group_text , regex = group_regex ), url_matcher = url_match , handler = func , setup = setup , navigate = navigate , priority = priority , ) rules = self . scraper . rules if self . scraper else self . rules rules . append ( rule ) return func return wrapper","title":"select()"},{"location":"reference.html#dude.scraper.Scraper.shutdown","text":"Decorator to register a function to the shutdown events. Shutdown events are executed before terminating the application for cleaning up or closing resources like files and database sessions. Source code in dude/scraper.py def shutdown ( self ) -> Callable : \"\"\" Decorator to register a function to the shutdown events. Shutdown events are executed before terminating the application for cleaning up or closing resources like files and database sessions. \"\"\" def wrapper ( func : Callable ) -> Callable : if asyncio . iscoroutinefunction ( func ): self . has_async = True events = self . scraper . events if self . scraper else self . events events [ \"shutdown\" ] . append ( func ) return func return wrapper","title":"shutdown()"},{"location":"reference.html#dude.scraper.Scraper.start_requests","text":"Decorator to register custom Request objects. Source code in dude/scraper.py def start_requests ( self ) -> Callable : \"\"\" Decorator to register custom Request objects. \"\"\" from httpx import Request def wrapper ( func : Callable ) -> Callable : requests = self . scraper . requests if self . scraper else self . requests for request in func (): assert isinstance ( request , Request ) requests . append ( request ) return func return wrapper","title":"start_requests()"},{"location":"reference.html#dude.scraper.Scraper.startup","text":"Decorator to register a function to startup events. Startup events are executed before any actual scraping happens to, for example, setup databases, etc. Source code in dude/scraper.py def startup ( self ) -> Callable : \"\"\" Decorator to register a function to startup events. Startup events are executed before any actual scraping happens to, for example, setup databases, etc. \"\"\" def wrapper ( func : Callable ) -> Callable : if asyncio . iscoroutinefunction ( func ): self . has_async = True events = self . scraper . events if self . scraper else self . events events [ \"startup\" ] . append ( func ) return func return wrapper","title":"startup()"},{"location":"advanced/index.html","text":"Advanced Usage \u00b6 Dude has several useful features that allow users to control how the web scraper behaves.","title":"Advanced Usage"},{"location":"advanced/index.html#advanced-usage","text":"Dude has several useful features that allow users to control how the web scraper behaves.","title":"Advanced Usage"},{"location":"advanced/01_setup.html","text":"Setup \u00b6 Setup handlers are very useful when performing initial actions after loading a website for the first time. Setup handlers could perform any of the following: Login Click on dialogs buttons To create a Setup handler, you can pass setup=True parameter to @select() decorator. The only difference with Setup and normal element handler is that setup functions should accept 2 parameters, the element matched by the selector and the Page object (or WebDriver object in Selenium). Click on the annotations (+ sign) for more details. Python from dude import select @select ( text = \"I agree\" , setup = True ) # (1) def agree ( element , page ): \"\"\" Clicks \"I agree\" in order to use the website. \"\"\" with page . expect_navigation (): # (2) element . click () # (3) Finds an element containing the text \"I agree\". (Playwright) We expect that after clicking the element, it will navigate us to our page of interest. (Playwright) Perform click on the element. Info You can have multiple Setup steps, make sure to set the priority to run them in order.","title":"Setup"},{"location":"advanced/01_setup.html#setup","text":"Setup handlers are very useful when performing initial actions after loading a website for the first time. Setup handlers could perform any of the following: Login Click on dialogs buttons To create a Setup handler, you can pass setup=True parameter to @select() decorator. The only difference with Setup and normal element handler is that setup functions should accept 2 parameters, the element matched by the selector and the Page object (or WebDriver object in Selenium). Click on the annotations (+ sign) for more details. Python from dude import select @select ( text = \"I agree\" , setup = True ) # (1) def agree ( element , page ): \"\"\" Clicks \"I agree\" in order to use the website. \"\"\" with page . expect_navigation (): # (2) element . click () # (3) Finds an element containing the text \"I agree\". (Playwright) We expect that after clicking the element, it will navigate us to our page of interest. (Playwright) Perform click on the element. Info You can have multiple Setup steps, make sure to set the priority to run them in order.","title":"Setup"},{"location":"advanced/02_navigate.html","text":"Navigate \u00b6 Navigate handlers are used to move from page to page. To create a Navigate handler, you can pass navigate=True parameter to @select() decorator. Like Setup handlers, Navigate handlers should accept 2 parameters, the element matched by the selector and the Page object (or WebDriver object in Selenium). Click on the annotations (+ sign) for more details. Python from dude import select @select ( text = \"Next\" , navigate = True ) # (1) def next_page ( element , page ): \"\"\" Clicks the Next button/link to navigate to the next page. \"\"\" with page . expect_navigation (): # (2) element . click () # (3) Finds an element containing the text \"Next\". (Playwright) We expect that after clicking the element, it will navigate us to the next page. (Playwright) Perform click on the element. Info You can have multiple Navigate steps, make sure to set the priority to run them in order. When having multiple Navigate steps, only the first element found will be considered and all the succeeding selectors will be skipped.","title":"Navigate"},{"location":"advanced/02_navigate.html#navigate","text":"Navigate handlers are used to move from page to page. To create a Navigate handler, you can pass navigate=True parameter to @select() decorator. Like Setup handlers, Navigate handlers should accept 2 parameters, the element matched by the selector and the Page object (or WebDriver object in Selenium). Click on the annotations (+ sign) for more details. Python from dude import select @select ( text = \"Next\" , navigate = True ) # (1) def next_page ( element , page ): \"\"\" Clicks the Next button/link to navigate to the next page. \"\"\" with page . expect_navigation (): # (2) element . click () # (3) Finds an element containing the text \"Next\". (Playwright) We expect that after clicking the element, it will navigate us to the next page. (Playwright) Perform click on the element. Info You can have multiple Navigate steps, make sure to set the priority to run them in order. When having multiple Navigate steps, only the first element found will be considered and all the succeeding selectors will be skipped.","title":"Navigate"},{"location":"advanced/03_grouping.html","text":"Grouping Results \u00b6 When scraping a page containing a list of information, for example, containing URLs, titles and descriptions, it is important to know how data can be grouped together. By default, all scraped results are grouped by :root which is the root document. To specify grouping, pass group=<selector-for-grouping> to @select() decorator. In the example below, the results are grouped by an element with class custom-group . The matched selectors should be children of this element. Click on the annotations (+ sign) for more details. Python from dude import select @select ( css = \".title\" , group = \".custom-group\" ) # (1) def result_title ( element ): return { \"title\" : element . text_content ()} Group the results by the CSS selector .custom-group . You can also specify groups by using the @group() decorator and passing the argument selector=\"<selector-for-grouping>\" . Python from dude import group , select @group ( css = \".custom-group\" ) # (1) @select ( css = \".title\" ) def result_title ( element ): return { \"title\" : element . text_content ()} Group the results by the CSS selector .custom-group . Supported group selector types \u00b6 The @select() decorator does not only accept group but also group_css , group_xpath , group_text and group_regex . Please take note that group_css , group_xpath , group_text and group_regex are specific and group can contain any of these types. Python from dude import select @select ( css = \".title\" , group_css = \"<css-selector>\" ) # (1) @select ( css = \".title\" , group_xpath = \"<xpath-selector>\" ) # (2) @select ( css = \".title\" , group_text = \"<text-selector>\" ) # (3) @select ( css = \".title\" , group_regex = \"<regex-selector>\" ) # (4) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } Group CSS Selector Group XPath Selector Group Text Selector Group Regular Expression Selector It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence group -> css -> xpath -> text -> regex . Like the @select() decorator, the @group() decorator also accepts selector , css , xpath , text and regex . Similarly, css , xpath , text and regex are specific and selector can contain any of these types. Python from dude import select @group ( css = \"<css-selector>\" ) # (1) @select ( selector = \"<selector>\" ) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } CSS Selector It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence selector -> css -> xpath -> text -> regex . Why we need to group the results \u00b6 The group parameter or the @group() decorator has the advantage of making sure that items are in their correct group. Take for example the HTML source below, notice that in the second div , there is no description. HTML < div class = \"custom-group\" > < p class = \"title\" > Title 1 </ p > < p class = \"description\" > Description 1 </ p > </ div > < div class = \"custom-group\" > < p class = \"title\" > Title 2 </ p > </ div > < div class = \"custom-group\" > < p class = \"title\" > Title 3 </ p > < p class = \"description\" > Description 3 </ p > </ div > When the group is not specified, the default grouping will be used which will result in \" Description 3 \" being grouped with \" Title 2 \". Default Grouping [ { \"_page_number\": 1, // ... \"description\": \"Description 1\", \"title\": \"Title 1\" }, { \"_page_number\": 1, // ... \"description\": \"Description 3\", \"title\": \"Title 2\" }, { \"_page_number\": 1, // ... \"title\": \"Title 3\" } ] By specifying the group in @select(..., group=\".custom-group\") , we will be able to get a better result. Specified Grouping [ { \"_page_number\": 1, // ... \"description\": \"Description 1\", \"title\": \"Title 1\" }, { \"_page_number\": 1, // ... \"title\": \"Title 2\" }, { \"_page_number\": 1, // ... \"description\": \"Description 3\", \"title\": \"Title 3\" } ] Groups simplify how you write your script \u00b6 Info The examples below are both acceptable way to write a scraper. You have the option to choose how you write your script. A common way developers write scraper can be illustrated using this example below (see examples/single_handler.py for the complete script). While this works, it can be hard to maintain. Performing all actions in one function from dude import select @select ( css = \".custom-group\" ) def result_handler ( element ): \"\"\" Perform all the heavy-lifting in a single handler. \"\"\" data = {} url = element . query_selector ( \"a.url\" ) if url : data [ \"url\" ] = url . get_attribute ( \"href\" ) title = element . query_selector ( \".title\" ) if title : data [ \"title\" ] = title . text_content () description = element . query_selector ( \".description\" ) if description : data [ \"description\" ] = description . text_content () return data It will only require us to write 3 simple functions but is much easier to read as we don't have to deal with querying the child elements, ourselves. Separate handlers with grouping from dude import group , select @select ( css = \"a.url\" , group = \".custom-group\" ) def result_url ( element ): return { \"url\" : element . get_attribute ( \"href\" )} @select ( css = \".title\" , group = \".custom-group\" ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \".description\" , group = \".custom-group\" ) def result_description ( element ): return { \"description\" : element . text_content ()} When are @group() decorator and group parameter used by Dude \u00b6 If the group parameter is present, it will be used for grouping. If the group parameter is not present, the selector in the @group() decorator will be used for grouping. If both group parameter and @group() decorator are not present, the :root element will be used for grouping. Info Use @group() decorator when using multiple @select() decorators in one function in order to reduce repetition. Examples \u00b6 Grouping by @group() decorator: examples/group_decorator.py . Grouping by passing group parameter to @select() decorator: examples/group_in_select.py .","title":"Grouping Results"},{"location":"advanced/03_grouping.html#grouping-results","text":"When scraping a page containing a list of information, for example, containing URLs, titles and descriptions, it is important to know how data can be grouped together. By default, all scraped results are grouped by :root which is the root document. To specify grouping, pass group=<selector-for-grouping> to @select() decorator. In the example below, the results are grouped by an element with class custom-group . The matched selectors should be children of this element. Click on the annotations (+ sign) for more details. Python from dude import select @select ( css = \".title\" , group = \".custom-group\" ) # (1) def result_title ( element ): return { \"title\" : element . text_content ()} Group the results by the CSS selector .custom-group . You can also specify groups by using the @group() decorator and passing the argument selector=\"<selector-for-grouping>\" . Python from dude import group , select @group ( css = \".custom-group\" ) # (1) @select ( css = \".title\" ) def result_title ( element ): return { \"title\" : element . text_content ()} Group the results by the CSS selector .custom-group .","title":"Grouping Results"},{"location":"advanced/03_grouping.html#supported-group-selector-types","text":"The @select() decorator does not only accept group but also group_css , group_xpath , group_text and group_regex . Please take note that group_css , group_xpath , group_text and group_regex are specific and group can contain any of these types. Python from dude import select @select ( css = \".title\" , group_css = \"<css-selector>\" ) # (1) @select ( css = \".title\" , group_xpath = \"<xpath-selector>\" ) # (2) @select ( css = \".title\" , group_text = \"<text-selector>\" ) # (3) @select ( css = \".title\" , group_regex = \"<regex-selector>\" ) # (4) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } Group CSS Selector Group XPath Selector Group Text Selector Group Regular Expression Selector It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence group -> css -> xpath -> text -> regex . Like the @select() decorator, the @group() decorator also accepts selector , css , xpath , text and regex . Similarly, css , xpath , text and regex are specific and selector can contain any of these types. Python from dude import select @group ( css = \"<css-selector>\" ) # (1) @select ( selector = \"<selector>\" ) def handler ( element ): return { \"<key>\" : \"<value-extracted-from-element>\" } CSS Selector It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence selector -> css -> xpath -> text -> regex .","title":"Supported group selector types"},{"location":"advanced/03_grouping.html#why-we-need-to-group-the-results","text":"The group parameter or the @group() decorator has the advantage of making sure that items are in their correct group. Take for example the HTML source below, notice that in the second div , there is no description. HTML < div class = \"custom-group\" > < p class = \"title\" > Title 1 </ p > < p class = \"description\" > Description 1 </ p > </ div > < div class = \"custom-group\" > < p class = \"title\" > Title 2 </ p > </ div > < div class = \"custom-group\" > < p class = \"title\" > Title 3 </ p > < p class = \"description\" > Description 3 </ p > </ div > When the group is not specified, the default grouping will be used which will result in \" Description 3 \" being grouped with \" Title 2 \". Default Grouping [ { \"_page_number\": 1, // ... \"description\": \"Description 1\", \"title\": \"Title 1\" }, { \"_page_number\": 1, // ... \"description\": \"Description 3\", \"title\": \"Title 2\" }, { \"_page_number\": 1, // ... \"title\": \"Title 3\" } ] By specifying the group in @select(..., group=\".custom-group\") , we will be able to get a better result. Specified Grouping [ { \"_page_number\": 1, // ... \"description\": \"Description 1\", \"title\": \"Title 1\" }, { \"_page_number\": 1, // ... \"title\": \"Title 2\" }, { \"_page_number\": 1, // ... \"description\": \"Description 3\", \"title\": \"Title 3\" } ]","title":"Why we need to group the results"},{"location":"advanced/03_grouping.html#groups-simplify-how-you-write-your-script","text":"Info The examples below are both acceptable way to write a scraper. You have the option to choose how you write your script. A common way developers write scraper can be illustrated using this example below (see examples/single_handler.py for the complete script). While this works, it can be hard to maintain. Performing all actions in one function from dude import select @select ( css = \".custom-group\" ) def result_handler ( element ): \"\"\" Perform all the heavy-lifting in a single handler. \"\"\" data = {} url = element . query_selector ( \"a.url\" ) if url : data [ \"url\" ] = url . get_attribute ( \"href\" ) title = element . query_selector ( \".title\" ) if title : data [ \"title\" ] = title . text_content () description = element . query_selector ( \".description\" ) if description : data [ \"description\" ] = description . text_content () return data It will only require us to write 3 simple functions but is much easier to read as we don't have to deal with querying the child elements, ourselves. Separate handlers with grouping from dude import group , select @select ( css = \"a.url\" , group = \".custom-group\" ) def result_url ( element ): return { \"url\" : element . get_attribute ( \"href\" )} @select ( css = \".title\" , group = \".custom-group\" ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \".description\" , group = \".custom-group\" ) def result_description ( element ): return { \"description\" : element . text_content ()}","title":"Groups simplify how you write your script"},{"location":"advanced/03_grouping.html#when-are-group-decorator-and-group-parameter-used-by-dude","text":"If the group parameter is present, it will be used for grouping. If the group parameter is not present, the selector in the @group() decorator will be used for grouping. If both group parameter and @group() decorator are not present, the :root element will be used for grouping. Info Use @group() decorator when using multiple @select() decorators in one function in order to reduce repetition.","title":"When are @group() decorator and group parameter used by Dude"},{"location":"advanced/03_grouping.html#examples","text":"Grouping by @group() decorator: examples/group_decorator.py . Grouping by passing group parameter to @select() decorator: examples/group_in_select.py .","title":"Examples"},{"location":"advanced/04_url_pattern_matching.html","text":"URL Pattern Matching \u00b6 In order to make a handler function to run on specific websites, a url pattern parameter can be passed to @select() decorator. The url_match parameter should be valid Unix shell-style wildcards (see https://docs.python.org/3/library/fnmatch.html) or custom function that returns a boolean. The example below will only run if the URL of the current page matches *.com/* . Python from dude import select @select ( css = \".title\" , url_match = \"*.com/*\" ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \"a.url\" , url_match = lambda x : x . endswith ( \".html\" )) def result_url ( element ): return { \"url\" : element . get_attribute ( \"href\" )} Examples \u00b6 A more extensive example can be found at examples/url_pattern.py .","title":"URL Pattern Matching"},{"location":"advanced/04_url_pattern_matching.html#url-pattern-matching","text":"In order to make a handler function to run on specific websites, a url pattern parameter can be passed to @select() decorator. The url_match parameter should be valid Unix shell-style wildcards (see https://docs.python.org/3/library/fnmatch.html) or custom function that returns a boolean. The example below will only run if the URL of the current page matches *.com/* . Python from dude import select @select ( css = \".title\" , url_match = \"*.com/*\" ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \"a.url\" , url_match = lambda x : x . endswith ( \".html\" )) def result_url ( element ): return { \"url\" : element . get_attribute ( \"href\" )}","title":"URL Pattern Matching"},{"location":"advanced/04_url_pattern_matching.html#examples","text":"A more extensive example can be found at examples/url_pattern.py .","title":"Examples"},{"location":"advanced/05_prioritization.html","text":"Prioritization \u00b6 Handlers are sorted based on the following sequence: Group Selector Priority If all handlers have the same priority value, they will be executed based on which handler was inserted first into the rule list. This arrangement depends on how handlers are defined inside python files and which python files was imported first. If no priority was provided to @select() decorator, the value defaults to 100. The example below makes sure that result_description() will be called first before result_title() . Python from dude import select @select ( css = \".title\" , priority = 1 ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \".description\" , priority = 0 ) def result_description ( element ): return { \"description\" : element . text_content ()} The priority value is most useful on Setup and Navigate handlers. As an example below, the selector css=\"#pnnext\" will be queried first before looking for text=\"Next\" . Take note that if css=\"#pnnext\" exists, then text=\"Next\" will not be queried anymore. Python from dude import select @select ( text = \"Next\" , navigate = True ) @select ( css = \"#pnnext\" , navigate = True , priority = 0 ) def next_page ( element , page ): with page . expect_navigation (): element . click () Examples \u00b6 A more extensive example can be found at examples/priority.py .","title":"Prioritization"},{"location":"advanced/05_prioritization.html#prioritization","text":"Handlers are sorted based on the following sequence: Group Selector Priority If all handlers have the same priority value, they will be executed based on which handler was inserted first into the rule list. This arrangement depends on how handlers are defined inside python files and which python files was imported first. If no priority was provided to @select() decorator, the value defaults to 100. The example below makes sure that result_description() will be called first before result_title() . Python from dude import select @select ( css = \".title\" , priority = 1 ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \".description\" , priority = 0 ) def result_description ( element ): return { \"description\" : element . text_content ()} The priority value is most useful on Setup and Navigate handlers. As an example below, the selector css=\"#pnnext\" will be queried first before looking for text=\"Next\" . Take note that if css=\"#pnnext\" exists, then text=\"Next\" will not be queried anymore. Python from dude import select @select ( text = \"Next\" , navigate = True ) @select ( css = \"#pnnext\" , navigate = True , priority = 0 ) def next_page ( element , page ): with page . expect_navigation (): element . click ()","title":"Prioritization"},{"location":"advanced/05_prioritization.html#examples","text":"A more extensive example can be found at examples/priority.py .","title":"Examples"},{"location":"advanced/06_custom_storage.html","text":"Custom Storage \u00b6 Dude currently support json , yaml/yml and csv formats only. However, this can be extended to support a custom storage or override the existing formats using the @save() decorator. The save function should accept 2 parameters, data (list of dictionary of scraped data) and optional output (can be filename or None ). Take note that the save function must return a boolean for success. The example below prints the output to terminal using tabulate for illustration purposes only. You can use the @save() decorator in other ways like saving the scraped data to spreadsheets, database or send it to an API. Python from dude import save import tabulate @save ( \"table\" ) def save_table ( data , output ) -> bool : \"\"\" Prints data to stdout using tabulate. \"\"\" print ( tabulate . tabulate ( tabular_data = data , headers = \"keys\" , maxcolwidths = 50 )) return True The custom storage above can then be called using any of the options below. Terminal Python dude scrape --url \"<url>\" path/to/script.py --format table if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"<url>\" ], pages = 2 , format = \"table\" ) Saving on every page \u00b6 It is possible to call the save functions after each page. This is useful when running in spider mode to prevent lost of data. To make use of this option, the flag is_per_page in the @save() should be set to True . Python @save ( \"table\" , is_per_page = True ) def save_table ( data , output ) -> bool : ... To run the scraper in per-page save, pass --save-per-page argument. Terminal Python dude scrape --url \"<url>\" path/to/script.py --format table --save-per-page if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"<url>\" ], pages = 2 , format = \"table\" , save_per_page = True ) Note The option --save-per-page is best used with events to make sure that connections or file handles are opened and closed properly. Check the examples below. Examples \u00b6 A more extensive example can be found at examples/custom_storage.py and examples/save_per_page.py .","title":"Custom Storage"},{"location":"advanced/06_custom_storage.html#custom-storage","text":"Dude currently support json , yaml/yml and csv formats only. However, this can be extended to support a custom storage or override the existing formats using the @save() decorator. The save function should accept 2 parameters, data (list of dictionary of scraped data) and optional output (can be filename or None ). Take note that the save function must return a boolean for success. The example below prints the output to terminal using tabulate for illustration purposes only. You can use the @save() decorator in other ways like saving the scraped data to spreadsheets, database or send it to an API. Python from dude import save import tabulate @save ( \"table\" ) def save_table ( data , output ) -> bool : \"\"\" Prints data to stdout using tabulate. \"\"\" print ( tabulate . tabulate ( tabular_data = data , headers = \"keys\" , maxcolwidths = 50 )) return True The custom storage above can then be called using any of the options below. Terminal Python dude scrape --url \"<url>\" path/to/script.py --format table if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"<url>\" ], pages = 2 , format = \"table\" )","title":"Custom Storage"},{"location":"advanced/06_custom_storage.html#saving-on-every-page","text":"It is possible to call the save functions after each page. This is useful when running in spider mode to prevent lost of data. To make use of this option, the flag is_per_page in the @save() should be set to True . Python @save ( \"table\" , is_per_page = True ) def save_table ( data , output ) -> bool : ... To run the scraper in per-page save, pass --save-per-page argument. Terminal Python dude scrape --url \"<url>\" path/to/script.py --format table --save-per-page if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"<url>\" ], pages = 2 , format = \"table\" , save_per_page = True ) Note The option --save-per-page is best used with events to make sure that connections or file handles are opened and closed properly. Check the examples below.","title":"Saving on every page"},{"location":"advanced/06_custom_storage.html#examples","text":"A more extensive example can be found at examples/custom_storage.py and examples/save_per_page.py .","title":"Examples"},{"location":"advanced/07_the_scraper_application_class.html","text":"The Scraper Application Class \u00b6 The decorators @select() and @save() and the function run() simplifies the usage of the framework. It is possible to create your own scraper application object using the example below. Warning \ud83d\udea8 This is not currently supported by the command-line interface! Please use the command python path/to/script.py to run the scraper application. Python from dude import Scraper app = Scraper () @app . select ( css = \".title\" ) def result_title ( element ): return { \"title\" : element . text_content ()} if __name__ == '__main__' : app . run ( urls = [ \"https://dude.ron.sh/\" ]) Examples \u00b6 A more extensive example can be found at examples/application.py .","title":"The Scraper Application Class"},{"location":"advanced/07_the_scraper_application_class.html#the-scraper-application-class","text":"The decorators @select() and @save() and the function run() simplifies the usage of the framework. It is possible to create your own scraper application object using the example below. Warning \ud83d\udea8 This is not currently supported by the command-line interface! Please use the command python path/to/script.py to run the scraper application. Python from dude import Scraper app = Scraper () @app . select ( css = \".title\" ) def result_title ( element ): return { \"title\" : element . text_content ()} if __name__ == '__main__' : app . run ( urls = [ \"https://dude.ron.sh/\" ])","title":"The Scraper Application Class"},{"location":"advanced/07_the_scraper_application_class.html#examples","text":"A more extensive example can be found at examples/application.py .","title":"Examples"},{"location":"advanced/08_async.html","text":"Asynchronous Support \u00b6 Handler functions can be converted to async. It is not possible to mix async and sync handlers since Playwright does not support this. It is however, possible to have async and sync storage handlers at the same time since this is not connected to Playwright anymore. Python from dude import save , select @select ( css = \".title\" ) async def result_title ( element ): return { \"title\" : await element . text_content ()} @save ( \"json\" ) async def save_json ( data , output ) -> bool : ... return True @save ( \"xml\" ) def save_xml ( data , output ) -> bool : # (1) ... return True Sync storage handler can be used on sync and async mode Examples \u00b6 A more extensive example can be found at examples/async.py .","title":"Asynchronous Support"},{"location":"advanced/08_async.html#asynchronous-support","text":"Handler functions can be converted to async. It is not possible to mix async and sync handlers since Playwright does not support this. It is however, possible to have async and sync storage handlers at the same time since this is not connected to Playwright anymore. Python from dude import save , select @select ( css = \".title\" ) async def result_title ( element ): return { \"title\" : await element . text_content ()} @save ( \"json\" ) async def save_json ( data , output ) -> bool : ... return True @save ( \"xml\" ) def save_xml ( data , output ) -> bool : # (1) ... return True Sync storage handler can be used on sync and async mode","title":"Asynchronous Support"},{"location":"advanced/08_async.html#examples","text":"A more extensive example can be found at examples/async.py .","title":"Examples"},{"location":"advanced/09_beautifulsoup4.html","text":"BeautifulSoup4 Scraper \u00b6 Option to use BeautifulSoup4 as parser backend instead of Playwright has been added in Release 0.2.0 . BeautifulSoup4 is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ bs4 ] Required changes to your script in order to use BeautifulSoup4 \u00b6 Instead of ElementHandle objects when using Playwright as parser backend, Soup objects are passed to the decorated functions. Python from dude import select @select ( css = \"a.url\" ) def result_url ( soup ): return { \"url\" : soup [ \"href\" ]} # (1) @select ( css = \".title\" ) def result_title ( soup ): return { \"title\" : soup . get_text ()} # (2) Attributes can be accessed by key. Texts can be accessed using the get_text() method. Running Dude with BeautifulSoup4 \u00b6 You can run BeautifulSoup4 parser backend using the --bs4 command-line argument or parser=\"bs4\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --bs4 --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"bs4\" , output = \"data.json\" ) Limitations \u00b6 BeautifulSoup4 only supports CSS selector. Setup handlers are not supported. Navigate handlers are not supported. Examples \u00b6 Examples are can be found at examples/bs4_sync.py and examples/bs4_async.py .","title":"BeautifulSoup4 Scraper"},{"location":"advanced/09_beautifulsoup4.html#beautifulsoup4-scraper","text":"Option to use BeautifulSoup4 as parser backend instead of Playwright has been added in Release 0.2.0 . BeautifulSoup4 is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ bs4 ]","title":"BeautifulSoup4 Scraper"},{"location":"advanced/09_beautifulsoup4.html#required-changes-to-your-script-in-order-to-use-beautifulsoup4","text":"Instead of ElementHandle objects when using Playwright as parser backend, Soup objects are passed to the decorated functions. Python from dude import select @select ( css = \"a.url\" ) def result_url ( soup ): return { \"url\" : soup [ \"href\" ]} # (1) @select ( css = \".title\" ) def result_title ( soup ): return { \"title\" : soup . get_text ()} # (2) Attributes can be accessed by key. Texts can be accessed using the get_text() method.","title":"Required changes to your script in order to use BeautifulSoup4"},{"location":"advanced/09_beautifulsoup4.html#running-dude-with-beautifulsoup4","text":"You can run BeautifulSoup4 parser backend using the --bs4 command-line argument or parser=\"bs4\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --bs4 --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"bs4\" , output = \"data.json\" )","title":"Running Dude with BeautifulSoup4"},{"location":"advanced/09_beautifulsoup4.html#limitations","text":"BeautifulSoup4 only supports CSS selector. Setup handlers are not supported. Navigate handlers are not supported.","title":"Limitations"},{"location":"advanced/09_beautifulsoup4.html#examples","text":"Examples are can be found at examples/bs4_sync.py and examples/bs4_async.py .","title":"Examples"},{"location":"advanced/10_parsel.html","text":"Parsel Scraper \u00b6 Option to use Parsel as parser backend instead of Playwright has been added in Release 0.5.0 . Parsel is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ parsel ] Required changes to your script in order to use Parsel \u00b6 Instead of ElementHandle objects when using Playwright as parser backend, Selector objects are passed to the decorated functions. Python from dude import select @select ( css = \"a.url::attr(href)\" ) # (1) def result_url ( selector ): return { \"url\" : selector . get ()} # (2) @select ( css = \".title::text\" ) # (3) def result_title ( selector ): return { \"title\" : selector . get ()} Attributes can be accessed by CSS non-standard pseudo-element, ::attr(name) . Values from Selector objects can be accessed using .get() method. Texts can be accessed by CSS non-standard pseudo-element, ::text . Running Dude with Parsel \u00b6 You can run Parsel parser backend using the --parsel command-line argument or parser=\"parsel\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --parsel --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"parsel\" , output = \"data.json\" ) Limitations \u00b6 Setup handlers are not supported. Navigate handlers are not supported. Examples \u00b6 Examples are can be found at examples/parsel_sync.py and examples/parsel_async.py .","title":"Parsel Scraper"},{"location":"advanced/10_parsel.html#parsel-scraper","text":"Option to use Parsel as parser backend instead of Playwright has been added in Release 0.5.0 . Parsel is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ parsel ]","title":"Parsel Scraper"},{"location":"advanced/10_parsel.html#required-changes-to-your-script-in-order-to-use-parsel","text":"Instead of ElementHandle objects when using Playwright as parser backend, Selector objects are passed to the decorated functions. Python from dude import select @select ( css = \"a.url::attr(href)\" ) # (1) def result_url ( selector ): return { \"url\" : selector . get ()} # (2) @select ( css = \".title::text\" ) # (3) def result_title ( selector ): return { \"title\" : selector . get ()} Attributes can be accessed by CSS non-standard pseudo-element, ::attr(name) . Values from Selector objects can be accessed using .get() method. Texts can be accessed by CSS non-standard pseudo-element, ::text .","title":"Required changes to your script in order to use Parsel"},{"location":"advanced/10_parsel.html#running-dude-with-parsel","text":"You can run Parsel parser backend using the --parsel command-line argument or parser=\"parsel\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --parsel --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"parsel\" , output = \"data.json\" )","title":"Running Dude with Parsel"},{"location":"advanced/10_parsel.html#limitations","text":"Setup handlers are not supported. Navigate handlers are not supported.","title":"Limitations"},{"location":"advanced/10_parsel.html#examples","text":"Examples are can be found at examples/parsel_sync.py and examples/parsel_async.py .","title":"Examples"},{"location":"advanced/11_lxml.html","text":"lxml Scraper \u00b6 Option to use lxml as parser backend instead of Playwright has been added in Release 0.6.0 . lxml is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ lxml ] Required changes to your script in order to use lxml \u00b6 Instead of ElementHandle objects when using Playwright as parser backend, Element, \"smart\" strings, etc. objects are passed to the decorated functions. Python from dude import select @select ( xpath = './/a[contains(@class, \"url\")]/@href' ) # (1) def result_url ( href ): return { \"url\" : href } # (2) @select ( css = \"a.url\" ) # (3) def result_url_css ( element ): return { \"url_css\" : element . attrib [ \"href\" ]} # (4) @select ( css = '.title' ) def result_title ( element ): return { \"title\" : element . text } # (5) Attributes can be accessed using XPath @href . When using XPath @href (or text ), \"smart\" strings are returned. The lxml backend supports CSS selectors via cssselect . Attributes can also be accessed from lxml elements using element.attrib[\"href\"] . Text content can be accessed from lxml elements using element.text . Running Dude with lxml \u00b6 You can run lxml parser backend using the --lxml command-line argument or parser=\"lxml\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --lxml --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"lxml\" , output = \"data.json\" ) Limitations \u00b6 Setup handlers are not supported. Navigate handlers are not supported. Examples \u00b6 Examples are can be found at examples/lxml_sync.py and examples/lxml_async.py .","title":"lxml Scraper"},{"location":"advanced/11_lxml.html#lxml-scraper","text":"Option to use lxml as parser backend instead of Playwright has been added in Release 0.6.0 . lxml is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ lxml ]","title":"lxml Scraper"},{"location":"advanced/11_lxml.html#required-changes-to-your-script-in-order-to-use-lxml","text":"Instead of ElementHandle objects when using Playwright as parser backend, Element, \"smart\" strings, etc. objects are passed to the decorated functions. Python from dude import select @select ( xpath = './/a[contains(@class, \"url\")]/@href' ) # (1) def result_url ( href ): return { \"url\" : href } # (2) @select ( css = \"a.url\" ) # (3) def result_url_css ( element ): return { \"url_css\" : element . attrib [ \"href\" ]} # (4) @select ( css = '.title' ) def result_title ( element ): return { \"title\" : element . text } # (5) Attributes can be accessed using XPath @href . When using XPath @href (or text ), \"smart\" strings are returned. The lxml backend supports CSS selectors via cssselect . Attributes can also be accessed from lxml elements using element.attrib[\"href\"] . Text content can be accessed from lxml elements using element.text .","title":"Required changes to your script in order to use lxml"},{"location":"advanced/11_lxml.html#running-dude-with-lxml","text":"You can run lxml parser backend using the --lxml command-line argument or parser=\"lxml\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --lxml --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"lxml\" , output = \"data.json\" )","title":"Running Dude with lxml"},{"location":"advanced/11_lxml.html#limitations","text":"Setup handlers are not supported. Navigate handlers are not supported.","title":"Limitations"},{"location":"advanced/11_lxml.html#examples","text":"Examples are can be found at examples/lxml_sync.py and examples/lxml_async.py .","title":"Examples"},{"location":"advanced/12_pyppeteer.html","text":"Pyppeteer Scraper \u00b6 Option to use Pyppeteer as parser backend instead of Playwright has been added in Release 0.8.0 . Pyppeteer is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ pyppeteer ] pyppeteer-install # (1) Download recent version of Chromium Required changes to your script in order to use Pyppeteer \u00b6 Instead of Playwright's ElementHandle objects when using Playwright as parser backend, Pyppeteer has its own ElementHandle objects that are passed to the decorated functions. The decorated functions will need to accept 2 arguments, element and page objects. This is needed because Pyppeteer elements does not expose a convenient function to get the text content. Info Pyppeteer only supports async Python from dude import select @select ( css = \"a.url\" ) async def result_url ( element , page ): # (1) handle = await element . getProperty ( \"href\" ) # (2) return { \"url\" : await handle . jsonValue ()} # (3) @select ( css = \".title\" ) async def result_title ( element , page ): return { \"title\" : await page . evaluate ( \"(element) => element.textContent\" , element )} # (4) In addition to element objects, page objects are also needed. Attributes/Properties can be accessed using getProperty() . jsonValue() is used to convert Pyppeteer objects to Python types. Page.evaluate() is used to get the element's text content. Running Dude with Pyppeteer \u00b6 You can run Pyppeteer parser backend using the --pyppeteer command-line argument or parser=\"pyppeteer\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --pyppeteer --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"pyppeteer\" , output = \"data.json\" ) Limitations \u00b6 Pyppeteer only supports async. Pyppeteer does not support XPath 2.0, therefore not allowing regular expression. Examples \u00b6 Examples are can be found at examples/pyppeteer_async.py .","title":"Pyppeteer Scraper"},{"location":"advanced/12_pyppeteer.html#pyppeteer-scraper","text":"Option to use Pyppeteer as parser backend instead of Playwright has been added in Release 0.8.0 . Pyppeteer is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ pyppeteer ] pyppeteer-install # (1) Download recent version of Chromium","title":"Pyppeteer Scraper"},{"location":"advanced/12_pyppeteer.html#required-changes-to-your-script-in-order-to-use-pyppeteer","text":"Instead of Playwright's ElementHandle objects when using Playwright as parser backend, Pyppeteer has its own ElementHandle objects that are passed to the decorated functions. The decorated functions will need to accept 2 arguments, element and page objects. This is needed because Pyppeteer elements does not expose a convenient function to get the text content. Info Pyppeteer only supports async Python from dude import select @select ( css = \"a.url\" ) async def result_url ( element , page ): # (1) handle = await element . getProperty ( \"href\" ) # (2) return { \"url\" : await handle . jsonValue ()} # (3) @select ( css = \".title\" ) async def result_title ( element , page ): return { \"title\" : await page . evaluate ( \"(element) => element.textContent\" , element )} # (4) In addition to element objects, page objects are also needed. Attributes/Properties can be accessed using getProperty() . jsonValue() is used to convert Pyppeteer objects to Python types. Page.evaluate() is used to get the element's text content.","title":"Required changes to your script in order to use Pyppeteer"},{"location":"advanced/12_pyppeteer.html#running-dude-with-pyppeteer","text":"You can run Pyppeteer parser backend using the --pyppeteer command-line argument or parser=\"pyppeteer\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --pyppeteer --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"pyppeteer\" , output = \"data.json\" )","title":"Running Dude with Pyppeteer"},{"location":"advanced/12_pyppeteer.html#limitations","text":"Pyppeteer only supports async. Pyppeteer does not support XPath 2.0, therefore not allowing regular expression.","title":"Limitations"},{"location":"advanced/12_pyppeteer.html#examples","text":"Examples are can be found at examples/pyppeteer_async.py .","title":"Examples"},{"location":"advanced/13_selenium.html","text":"Selenium Scraper \u00b6 Option to use Selenium as parser backend instead of Playwright has been added in Release 0.9.0 . Selenium is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ selenium ] Required changes to your script in order to use Selenium \u00b6 Instead of Playwright's ElementHandle objects when using Playwright as parser backend, WebElement objects are passed to the decorated functions. Python from dude import select @select ( css = \"a.url\" ) def result_url ( element , page ): return { \"url\" : element . get_attribute ( \"href\" )} @select ( css = \".title\" ) def result_title ( element , page ): return { \"title\" : element . text } Running Dude with Selenium \u00b6 You can run Selenium parser backend using the --selenium command-line argument or parser=\"selenium\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --selenium --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"selenium\" , output = \"data.json\" ) Limitations \u00b6 Selenium does not support XPath 2.0, therefore not allowing regular expression. Examples \u00b6 Examples are can be found at examples/selenium_sync.py and examples/selenium_async.py .","title":"Selenium Scraper"},{"location":"advanced/13_selenium.html#selenium-scraper","text":"Option to use Selenium as parser backend instead of Playwright has been added in Release 0.9.0 . Selenium is an optional dependency and can only be installed via pip using the command below. Terminal pip install pydude [ selenium ]","title":"Selenium Scraper"},{"location":"advanced/13_selenium.html#required-changes-to-your-script-in-order-to-use-selenium","text":"Instead of Playwright's ElementHandle objects when using Playwright as parser backend, WebElement objects are passed to the decorated functions. Python from dude import select @select ( css = \"a.url\" ) def result_url ( element , page ): return { \"url\" : element . get_attribute ( \"href\" )} @select ( css = \".title\" ) def result_title ( element , page ): return { \"title\" : element . text }","title":"Required changes to your script in order to use Selenium"},{"location":"advanced/13_selenium.html#running-dude-with-selenium","text":"You can run Selenium parser backend using the --selenium command-line argument or parser=\"selenium\" parameter to run() . Terminal Python dude scrape --url \"<url>\" --selenium --output data.json path/to/script.py if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh/\" ], parser = \"selenium\" , output = \"data.json\" )","title":"Running Dude with Selenium"},{"location":"advanced/13_selenium.html#limitations","text":"Selenium does not support XPath 2.0, therefore not allowing regular expression.","title":"Limitations"},{"location":"advanced/13_selenium.html#examples","text":"Examples are can be found at examples/selenium_sync.py and examples/selenium_async.py .","title":"Examples"},{"location":"advanced/14_events.html","text":"Events \u00b6 Functions can be registered to be called on specific events which makes it possible to run custom actions like setting up databases or calling API for authentication, and performing additional actions on page objects (can be soup, driver, selector or tree objects) like taking screenshots. Here is a diagram when events are being executed. Startup Event \u00b6 The Startup event is executed at the start of the process. The @startup() decorator can be used to register a function for startup. This can be used to setup databases or authenticate to APIs and other possible use-cases prior to actual web scraping. from pathlib import Path from dude import startup SAVE_DIR : Path @startup () def initialize_csv (): global SAVE_DIR SAVE_DIR = Path ( __file__ ) . resolve () . parent / \"temp\" SAVE_DIR . mkdir ( exist_ok = True ) Pre-Setup Event \u00b6 The Pre-Setup event is executed after loading a page or getting an HTTP response. The @pre_setup() decorator can be used to register a function for pre-setup. Note that the function should accept one argument which can either be a page, driver, soup, Parsel selector or LXML tree. import uuid from dude import pre_setup ... @pre_setup () def screenshot ( page ): unique_name = str ( uuid . uuid4 ()) page . screenshot ( path = SAVE_DIR / f \" { unique_name } .png\" ) Post-Setup Event \u00b6 The Post-Setup event is executed after running the setup functions . The @post_setup() decorator can be used to register a function for post-setup. Note that the function should accept one argument which can either be a page, driver, soup, Parsel selector or LXML tree. import uuid from dude import post_setup ... @post_setup () def print_pdf ( page ): unique_name = str ( uuid . uuid4 ()) page . pdf ( path = SAVE_DIR / f \" { unique_name } .pdf\" ) Shutdown Event \u00b6 The Shutdown event is executed before terminating the application. The @shutdown() decorator can be used to register a function for shutdown. import shutil from dude import shutdown @shutdown () def zip_all (): shutil . make_archive ( \"images-and-pdfs\" , \"zip\" , SAVE_DIR )","title":"Events"},{"location":"advanced/14_events.html#events","text":"Functions can be registered to be called on specific events which makes it possible to run custom actions like setting up databases or calling API for authentication, and performing additional actions on page objects (can be soup, driver, selector or tree objects) like taking screenshots. Here is a diagram when events are being executed.","title":"Events"},{"location":"advanced/14_events.html#startup-event","text":"The Startup event is executed at the start of the process. The @startup() decorator can be used to register a function for startup. This can be used to setup databases or authenticate to APIs and other possible use-cases prior to actual web scraping. from pathlib import Path from dude import startup SAVE_DIR : Path @startup () def initialize_csv (): global SAVE_DIR SAVE_DIR = Path ( __file__ ) . resolve () . parent / \"temp\" SAVE_DIR . mkdir ( exist_ok = True )","title":"Startup Event"},{"location":"advanced/14_events.html#pre-setup-event","text":"The Pre-Setup event is executed after loading a page or getting an HTTP response. The @pre_setup() decorator can be used to register a function for pre-setup. Note that the function should accept one argument which can either be a page, driver, soup, Parsel selector or LXML tree. import uuid from dude import pre_setup ... @pre_setup () def screenshot ( page ): unique_name = str ( uuid . uuid4 ()) page . screenshot ( path = SAVE_DIR / f \" { unique_name } .png\" )","title":"Pre-Setup Event"},{"location":"advanced/14_events.html#post-setup-event","text":"The Post-Setup event is executed after running the setup functions . The @post_setup() decorator can be used to register a function for post-setup. Note that the function should accept one argument which can either be a page, driver, soup, Parsel selector or LXML tree. import uuid from dude import post_setup ... @post_setup () def print_pdf ( page ): unique_name = str ( uuid . uuid4 ()) page . pdf ( path = SAVE_DIR / f \" { unique_name } .pdf\" )","title":"Post-Setup Event"},{"location":"advanced/14_events.html#shutdown-event","text":"The Shutdown event is executed before terminating the application. The @shutdown() decorator can be used to register a function for shutdown. import shutil from dude import shutdown @shutdown () def zip_all (): shutil . make_archive ( \"images-and-pdfs\" , \"zip\" , SAVE_DIR )","title":"Shutdown Event"},{"location":"advanced/15_start_requests.html","text":"@start_requests decorator \u00b6 The @start_requests decorator adds an option to send custom HTTP methods (POST, PUT, PATCH, etc). Warning @start_requests() is only supported on BeautifulSoup4, lxml and Parsel backends. To register custom Request objects, simply wrap a generator with @start_requests decorator. Click on the annotations (+ sign) for more details. Python from dude import Request # (1) @start_requests () def custom_requests (): for url in [ \"https://dude.ron.sh\" ]: yield Request ( method = \"GET\" , url = url ) # (2) @select ( css = \"a.url\" ) def result_url ( soup ): return { \"url\" : soup [ \"href\" ]} if __name__ == \"__main__\" : import dude dude . run ( urls = [], parser = \"bs4\" ) # (3) Import the Request class. It is necessary to specify the HTTP method. url param should be set to an empty list if not needed.","title":"@start_requests"},{"location":"advanced/15_start_requests.html#start_requests-decorator","text":"The @start_requests decorator adds an option to send custom HTTP methods (POST, PUT, PATCH, etc). Warning @start_requests() is only supported on BeautifulSoup4, lxml and Parsel backends. To register custom Request objects, simply wrap a generator with @start_requests decorator. Click on the annotations (+ sign) for more details. Python from dude import Request # (1) @start_requests () def custom_requests (): for url in [ \"https://dude.ron.sh\" ]: yield Request ( method = \"GET\" , url = url ) # (2) @select ( css = \"a.url\" ) def result_url ( soup ): return { \"url\" : soup [ \"href\" ]} if __name__ == \"__main__\" : import dude dude . run ( urls = [], parser = \"bs4\" ) # (3) Import the Request class. It is necessary to specify the HTTP method. url param should be set to an empty list if not needed.","title":"@start_requests decorator"},{"location":"supported_parser_backends/index.html","text":"Supported Parser Backends \u00b6 By default, Dude uses Playwright but gives you an option to use parser backends that you are familiar with. It is possible to use parser backends like BeautifulSoup4 , Parsel and lxml . Here is the summary of features supported by each parser backend. Parser Backend Supports Sync? Supports Async? Selectors Setup Handler Navigate Handler CSS XPath Text Regex Playwright \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 BeautifulSoup4 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab Parsel \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab lxml \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab Pyppeteer \ud83d\udeab \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \u2705 \u2705 Selenium \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \u2705 \u2705","title":"Supported Parser Backends"},{"location":"supported_parser_backends/index.html#supported-parser-backends","text":"By default, Dude uses Playwright but gives you an option to use parser backends that you are familiar with. It is possible to use parser backends like BeautifulSoup4 , Parsel and lxml . Here is the summary of features supported by each parser backend. Parser Backend Supports Sync? Supports Async? Selectors Setup Handler Navigate Handler CSS XPath Text Regex Playwright \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 BeautifulSoup4 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab Parsel \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab lxml \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab Pyppeteer \ud83d\udeab \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \u2705 \u2705 Selenium \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \u2705 \u2705","title":"Supported Parser Backends"},{"location":"supported_parser_backends/migrating.html","text":"Migrating Your Web Scrapers to Dude \u00b6 Here are examples showing how web scrapers are commonly written compared to how they will be when written in Dude. Playwright \u00b6 Example: Scrape Google search results Using pure Playwright Using Playwright with Dude import itertools import json from playwright.sync_api import sync_playwright def main ( urls , output , headless , pages ): results = [] with sync_playwright () as p : browser = p . chromium . launch ( headless = headless ) page = browser . new_page () for url in urls : page . goto ( url ) # click I agree with page . expect_navigation (): page . locator ( 'text=\"I agree\"' ) . click () for page_number in range ( 1 , pages + 1 ): for group in page . query_selector_all ( \".g\" ): url_elements = group . query_selector_all ( \"*css=a >> h3:nth-child(2)\" ) title_elements = group . query_selector_all ( \"h3:nth-child(2)\" ) description_elements = group . query_selector_all ( \"div[style='-webkit-line-clamp \\\\ 3A 2']\" ) # group together since each .g div can contain more than one set of results for url_element , title_element , description_element in itertools . zip_longest ( url_elements , title_elements , description_elements ): results . append ( { \"url\" : url_element . get_attribute ( \"href\" ) if url_element else None , \"title\" : title_element . text_content () if title_element else None , \"description\" : description_element . text_content () if description_element else None , \"page\" : page_number , } ) # go to next page with page . expect_navigation (): page . locator ( \"text=Next\" ) . click () browser . close () with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], output = \"data.json\" , headless = False , pages = 2 ) from dude import select @select ( selector = \"*css=a >> h3:nth-child(2)\" , group_css = \".g\" ) def result_url ( element ): return { \"url\" : element . get_attribute ( \"href\" )} @select ( css = \"h3:nth-child(2)\" , group_css = \".g\" ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \"div[style='-webkit-line-clamp \\\\ 3A 2']\" , group_css = \".g\" ) def result_description ( element ): return { \"description\" : element . text_content ()} @select ( text = \"I agree\" , setup = True ) def agree ( element , page ): with page . expect_navigation (): element . click () @select ( text = \"Next\" , navigate = True ) def next_page ( element , page ): with page . expect_navigation (): element . click () if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], output = \"data.json\" , headless = False , pages = 2 , ) Pyppeteer \u00b6 Example: Scrape Google search results Using pure Pyppeteer Using Pyppeteer with Dude import asyncio import itertools import json from pyppeteer import launch async def main ( urls , output , headless , pages ): results = [] launch_args = { \"headless\" : headless , \"args\" : [ \"--no-sandbox\" ]} browser = await launch ( options = launch_args ) page = await browser . newPage () for url in urls : await page . goto ( url ) # click I agree agree_button = await page . querySelector ( \"#L2AGLb > div\" ) await asyncio . gather ( page . waitForNavigation (), agree_button . click (), ) for page_number in range ( 1 , pages + 1 ): for group in await page . querySelectorAll ( \".g\" ): url_elements = await group . querySelectorAll ( \"a\" ) title_elements = await group . querySelectorAll ( \"h3:nth-child(2)\" ) description_elements = await group . querySelectorAll ( \"div[style='-webkit-line-clamp \\\\ 3A 2']\" ) # group together since each .g div can contain more than one set of results for url_element , title_element , description_element in itertools . zip_longest ( url_elements , title_elements , description_elements ): href = None if url_element : url_handle = await url_element . getProperty ( \"href\" ) href = await url_handle . jsonValue () title = None if title_element : title = await page . evaluate ( \"(element) => element.textContent\" , title_element ) description = None if description_element : description = await page . evaluate ( \"(element) => element.textContent\" , description_element ) results . append ( { \"url\" : href , \"title\" : title , \"description\" : description , \"page\" : page_number , } ) # go to next page next_element = await page . querySelector ( \"#pnnext\" ) await asyncio . gather ( page . waitForNavigation (), next_element . click (), ) await browser . close () with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : loop = asyncio . get_event_loop () loop . run_until_complete ( main ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], output = \"data.json\" , headless = False , pages = 2 , ) ) import asyncio from dude import select @select ( selector = \"a\" , group_css = \".g\" ) async def result_url ( element , page ): handle = await element . getProperty ( \"href\" ) return { \"url\" : await handle . jsonValue ()} @select ( css = \"h3:nth-child(2)\" , group_css = \".g\" ) async def result_title ( element , page ): return { \"title\" : await page . evaluate ( \"(element) => element.textContent\" , element )} @select ( css = \"div[style='-webkit-line-clamp \\\\ 3A 2']\" , group_css = \".g\" ) async def result_description ( element , page ): return { \"description\" : await page . evaluate ( \"(element) => element.textContent\" , element )} @select ( css = \"#L2AGLb > div\" , setup = True ) async def agree ( element , page ): await asyncio . gather ( page . waitForNavigation (), element . click (), ) @select ( css = \"#pnnext\" , navigate = True ) async def next_page ( element , page ): await asyncio . gather ( page . waitForNavigation (), element . click (), ) if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], parser = \"pyppeteer\" , output = \"data.json\" , headless = False , pages = 2 , ) BeautifulSoup4 \u00b6 Example: Get all links, titles and descriptions from https://dude.ron.sh Using HTTPX + BeautifulSoup4 Using BeautifulSoup4 with Dude import itertools import json import httpx from bs4 import BeautifulSoup def main ( urls , output ): results = [] with httpx . Client () as client : for url in urls : try : response = client . get ( url ) response . raise_for_status () content = response . text except httpx . HTTPStatusError as e : raise soup = BeautifulSoup ( content , \"html.parser\" ) for group in soup . select ( \".custom-group\" ): url_elements = group . select ( \"a.url\" ) title_elements = group . select ( \".title\" ) description_elements = group . select ( \".description\" ) # group together since each .custom-group div can contain more than one set of results for url_element , title_element , description_element in itertools . zip_longest ( url_elements , title_elements , description_elements ): results . append ( { \"url\" : url_element [ \"href\" ] if url_element else None , \"title\" : title_element . get_text () if title_element else None , \"description\" : description_element . get_text () if description_element else None , } ) with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://dude.ron.sh\" ], output = \"data.json\" ) from dude import select @select ( css = \"a.url\" , group_css = \".custom-group\" ) def result_url ( soup ): return { \"url\" : soup [ \"href\" ]} @select ( css = \".title\" , group_css = \".custom-group\" ) def result_title ( soup ): return { \"title\" : soup . get_text ()} @select ( css = \".description\" , group_css = \".custom-group\" ) def result_description ( soup ): return { \"description\" : soup . get_text ()} if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh\" ], parser = \"bs4\" , output = \"data.json\" ) Parsel \u00b6 Example: Get all links, titles and descriptions from https://dude.ron.sh Using HTTPX + Parsel Using Parsel with Dude import itertools import json import httpx from parsel import Selector def main ( urls , output ): results = [] with httpx . Client () as client : for url in urls : try : response = client . get ( url ) response . raise_for_status () content = response . text except httpx . HTTPStatusError as e : raise selector = Selector ( content ) for group in selector . css ( \".custom-group\" ): hrefs = group . css ( \"a.url::attr(href)\" ) titles = group . css ( \".title::text\" ) descriptions = group . css ( \".description::text\" ) # group together since each .custom-group div can contain more than one set of results for href , title , description in itertools . zip_longest ( hrefs , titles , descriptions ): results . append ( { \"url\" : href . get () if href else None , \"title\" : title . get () if title else None , \"description\" : description . get () if description else None , } ) with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://dude.ron.sh\" ], output = \"data.json\" ) from dude import select @select ( css = \"a.url::attr(href)\" , group_css = \".custom-group\" ) def result_url ( selector ): return { \"url\" : selector . get ()} @select ( css = \".title::text\" , group_css = \".custom-group\" ) def result_title ( selector ): return { \"title\" : selector . get ()} @select ( css = \".description::text\" , group_css = \".custom-group\" ) def result_description ( selector ): return { \"description\" : selector . get ()} if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh\" ], parser = \"parsel\" , output = \"data.json\" ) lxml \u00b6 Example: Get all links, titles and descriptions from https://dude.ron.sh Using HTTPX + lxml + cssselect Using lxml with Dude import itertools import json import httpx from lxml import etree def main ( urls , output ): results = [] with httpx . Client () as client : for url in urls : try : response = client . get ( url ) response . raise_for_status () content = response . text except httpx . HTTPStatusError as e : raise tree = etree . HTML ( text = content ) for group in tree . cssselect ( \".custom-group\" ): hrefs = group . xpath ( './/a[contains(@class, \"url\")]/@href' ) titles = group . xpath ( './/p[contains(@class, \"title\")]/text()' ) descriptions = group . xpath ( './/p[contains(@class, \"description\")]/text()' ) # group together since each .custom-group div can contain more than one set of results for href , title , description in itertools . zip_longest ( hrefs , titles , descriptions ): results . append ( { \"url\" : href , \"title\" : title , \"description\" : description , } ) with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://dude.ron.sh\" ], output = \"data.json\" ) from dude import select @select ( xpath = './/a[contains(@class, \"url\")]/@href' , group_css = \".custom-group\" ) def result_url ( href ): return { \"url\" : href } @select ( xpath = './/p[contains(@class, \"title\")]/text()' , group_css = \".custom-group\" ) def result_title ( text ): return { \"title\" : text } @select ( xpath = './/p[contains(@class, \"description\")]/text()' , group_css = \".custom-group\" ) def result_description ( text ): return { \"description\" : text } if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh\" ], parser = \"lxml\" , output = \"data.json\" )","title":"Migrating Your Web Scrapers to Dude"},{"location":"supported_parser_backends/migrating.html#migrating-your-web-scrapers-to-dude","text":"Here are examples showing how web scrapers are commonly written compared to how they will be when written in Dude.","title":"Migrating Your Web Scrapers to Dude"},{"location":"supported_parser_backends/migrating.html#playwright","text":"Example: Scrape Google search results Using pure Playwright Using Playwright with Dude import itertools import json from playwright.sync_api import sync_playwright def main ( urls , output , headless , pages ): results = [] with sync_playwright () as p : browser = p . chromium . launch ( headless = headless ) page = browser . new_page () for url in urls : page . goto ( url ) # click I agree with page . expect_navigation (): page . locator ( 'text=\"I agree\"' ) . click () for page_number in range ( 1 , pages + 1 ): for group in page . query_selector_all ( \".g\" ): url_elements = group . query_selector_all ( \"*css=a >> h3:nth-child(2)\" ) title_elements = group . query_selector_all ( \"h3:nth-child(2)\" ) description_elements = group . query_selector_all ( \"div[style='-webkit-line-clamp \\\\ 3A 2']\" ) # group together since each .g div can contain more than one set of results for url_element , title_element , description_element in itertools . zip_longest ( url_elements , title_elements , description_elements ): results . append ( { \"url\" : url_element . get_attribute ( \"href\" ) if url_element else None , \"title\" : title_element . text_content () if title_element else None , \"description\" : description_element . text_content () if description_element else None , \"page\" : page_number , } ) # go to next page with page . expect_navigation (): page . locator ( \"text=Next\" ) . click () browser . close () with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], output = \"data.json\" , headless = False , pages = 2 ) from dude import select @select ( selector = \"*css=a >> h3:nth-child(2)\" , group_css = \".g\" ) def result_url ( element ): return { \"url\" : element . get_attribute ( \"href\" )} @select ( css = \"h3:nth-child(2)\" , group_css = \".g\" ) def result_title ( element ): return { \"title\" : element . text_content ()} @select ( css = \"div[style='-webkit-line-clamp \\\\ 3A 2']\" , group_css = \".g\" ) def result_description ( element ): return { \"description\" : element . text_content ()} @select ( text = \"I agree\" , setup = True ) def agree ( element , page ): with page . expect_navigation (): element . click () @select ( text = \"Next\" , navigate = True ) def next_page ( element , page ): with page . expect_navigation (): element . click () if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], output = \"data.json\" , headless = False , pages = 2 , )","title":"Playwright"},{"location":"supported_parser_backends/migrating.html#pyppeteer","text":"Example: Scrape Google search results Using pure Pyppeteer Using Pyppeteer with Dude import asyncio import itertools import json from pyppeteer import launch async def main ( urls , output , headless , pages ): results = [] launch_args = { \"headless\" : headless , \"args\" : [ \"--no-sandbox\" ]} browser = await launch ( options = launch_args ) page = await browser . newPage () for url in urls : await page . goto ( url ) # click I agree agree_button = await page . querySelector ( \"#L2AGLb > div\" ) await asyncio . gather ( page . waitForNavigation (), agree_button . click (), ) for page_number in range ( 1 , pages + 1 ): for group in await page . querySelectorAll ( \".g\" ): url_elements = await group . querySelectorAll ( \"a\" ) title_elements = await group . querySelectorAll ( \"h3:nth-child(2)\" ) description_elements = await group . querySelectorAll ( \"div[style='-webkit-line-clamp \\\\ 3A 2']\" ) # group together since each .g div can contain more than one set of results for url_element , title_element , description_element in itertools . zip_longest ( url_elements , title_elements , description_elements ): href = None if url_element : url_handle = await url_element . getProperty ( \"href\" ) href = await url_handle . jsonValue () title = None if title_element : title = await page . evaluate ( \"(element) => element.textContent\" , title_element ) description = None if description_element : description = await page . evaluate ( \"(element) => element.textContent\" , description_element ) results . append ( { \"url\" : href , \"title\" : title , \"description\" : description , \"page\" : page_number , } ) # go to next page next_element = await page . querySelector ( \"#pnnext\" ) await asyncio . gather ( page . waitForNavigation (), next_element . click (), ) await browser . close () with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : loop = asyncio . get_event_loop () loop . run_until_complete ( main ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], output = \"data.json\" , headless = False , pages = 2 , ) ) import asyncio from dude import select @select ( selector = \"a\" , group_css = \".g\" ) async def result_url ( element , page ): handle = await element . getProperty ( \"href\" ) return { \"url\" : await handle . jsonValue ()} @select ( css = \"h3:nth-child(2)\" , group_css = \".g\" ) async def result_title ( element , page ): return { \"title\" : await page . evaluate ( \"(element) => element.textContent\" , element )} @select ( css = \"div[style='-webkit-line-clamp \\\\ 3A 2']\" , group_css = \".g\" ) async def result_description ( element , page ): return { \"description\" : await page . evaluate ( \"(element) => element.textContent\" , element )} @select ( css = \"#L2AGLb > div\" , setup = True ) async def agree ( element , page ): await asyncio . gather ( page . waitForNavigation (), element . click (), ) @select ( css = \"#pnnext\" , navigate = True ) async def next_page ( element , page ): await asyncio . gather ( page . waitForNavigation (), element . click (), ) if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://www.google.com/search?q=dude&hl=en\" ], parser = \"pyppeteer\" , output = \"data.json\" , headless = False , pages = 2 , )","title":"Pyppeteer"},{"location":"supported_parser_backends/migrating.html#beautifulsoup4","text":"Example: Get all links, titles and descriptions from https://dude.ron.sh Using HTTPX + BeautifulSoup4 Using BeautifulSoup4 with Dude import itertools import json import httpx from bs4 import BeautifulSoup def main ( urls , output ): results = [] with httpx . Client () as client : for url in urls : try : response = client . get ( url ) response . raise_for_status () content = response . text except httpx . HTTPStatusError as e : raise soup = BeautifulSoup ( content , \"html.parser\" ) for group in soup . select ( \".custom-group\" ): url_elements = group . select ( \"a.url\" ) title_elements = group . select ( \".title\" ) description_elements = group . select ( \".description\" ) # group together since each .custom-group div can contain more than one set of results for url_element , title_element , description_element in itertools . zip_longest ( url_elements , title_elements , description_elements ): results . append ( { \"url\" : url_element [ \"href\" ] if url_element else None , \"title\" : title_element . get_text () if title_element else None , \"description\" : description_element . get_text () if description_element else None , } ) with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://dude.ron.sh\" ], output = \"data.json\" ) from dude import select @select ( css = \"a.url\" , group_css = \".custom-group\" ) def result_url ( soup ): return { \"url\" : soup [ \"href\" ]} @select ( css = \".title\" , group_css = \".custom-group\" ) def result_title ( soup ): return { \"title\" : soup . get_text ()} @select ( css = \".description\" , group_css = \".custom-group\" ) def result_description ( soup ): return { \"description\" : soup . get_text ()} if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh\" ], parser = \"bs4\" , output = \"data.json\" )","title":"BeautifulSoup4"},{"location":"supported_parser_backends/migrating.html#parsel","text":"Example: Get all links, titles and descriptions from https://dude.ron.sh Using HTTPX + Parsel Using Parsel with Dude import itertools import json import httpx from parsel import Selector def main ( urls , output ): results = [] with httpx . Client () as client : for url in urls : try : response = client . get ( url ) response . raise_for_status () content = response . text except httpx . HTTPStatusError as e : raise selector = Selector ( content ) for group in selector . css ( \".custom-group\" ): hrefs = group . css ( \"a.url::attr(href)\" ) titles = group . css ( \".title::text\" ) descriptions = group . css ( \".description::text\" ) # group together since each .custom-group div can contain more than one set of results for href , title , description in itertools . zip_longest ( hrefs , titles , descriptions ): results . append ( { \"url\" : href . get () if href else None , \"title\" : title . get () if title else None , \"description\" : description . get () if description else None , } ) with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://dude.ron.sh\" ], output = \"data.json\" ) from dude import select @select ( css = \"a.url::attr(href)\" , group_css = \".custom-group\" ) def result_url ( selector ): return { \"url\" : selector . get ()} @select ( css = \".title::text\" , group_css = \".custom-group\" ) def result_title ( selector ): return { \"title\" : selector . get ()} @select ( css = \".description::text\" , group_css = \".custom-group\" ) def result_description ( selector ): return { \"description\" : selector . get ()} if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh\" ], parser = \"parsel\" , output = \"data.json\" )","title":"Parsel"},{"location":"supported_parser_backends/migrating.html#lxml","text":"Example: Get all links, titles and descriptions from https://dude.ron.sh Using HTTPX + lxml + cssselect Using lxml with Dude import itertools import json import httpx from lxml import etree def main ( urls , output ): results = [] with httpx . Client () as client : for url in urls : try : response = client . get ( url ) response . raise_for_status () content = response . text except httpx . HTTPStatusError as e : raise tree = etree . HTML ( text = content ) for group in tree . cssselect ( \".custom-group\" ): hrefs = group . xpath ( './/a[contains(@class, \"url\")]/@href' ) titles = group . xpath ( './/p[contains(@class, \"title\")]/text()' ) descriptions = group . xpath ( './/p[contains(@class, \"description\")]/text()' ) # group together since each .custom-group div can contain more than one set of results for href , title , description in itertools . zip_longest ( hrefs , titles , descriptions ): results . append ( { \"url\" : href , \"title\" : title , \"description\" : description , } ) with open ( output , \"w\" ) as f : json . dump ( results , f , indent = 2 ) if __name__ == \"__main__\" : main ( urls = [ \"https://dude.ron.sh\" ], output = \"data.json\" ) from dude import select @select ( xpath = './/a[contains(@class, \"url\")]/@href' , group_css = \".custom-group\" ) def result_url ( href ): return { \"url\" : href } @select ( xpath = './/p[contains(@class, \"title\")]/text()' , group_css = \".custom-group\" ) def result_title ( text ): return { \"title\" : text } @select ( xpath = './/p[contains(@class, \"description\")]/text()' , group_css = \".custom-group\" ) def result_description ( text ): return { \"description\" : text } if __name__ == \"__main__\" : import dude dude . run ( urls = [ \"https://dude.ron.sh\" ], parser = \"lxml\" , output = \"data.json\" )","title":"lxml"}]}